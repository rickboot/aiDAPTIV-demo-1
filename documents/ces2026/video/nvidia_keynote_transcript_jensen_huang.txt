NVIDIA CES 2026 KEYNOTE - JENSEN HUANG
Video Transcript | January 7, 2026 | 90 minutes
Source: YouTube (Official NVIDIA Channel)

[TRANSCRIPT BEGINS - 00:00:00]

JENSEN HUANG: Good morning, CES! Welcome to the future of AI computing. Today, I'm thrilled to unveil the Vera Rubin platform - our most ambitious AI infrastructure yet.

[Applause]

Let me start with a fundamental challenge facing our industry: AI is growing faster than memory can keep up. We've seen model sizes double every six months, but memory capacity? It's growing at maybe 20% per year. This is the defining constraint of the AI era.

[00:03:15]

The Vera Rubin platform addresses this head-on. Let me show you the architecture. [Slide: Vera Rubin System Diagram]

At the heart is the Rubin GPU - 50 petaflops of NVFP4 inference performance. But here's what's revolutionary: we've paired it with our Vera CPU, which delivers 1.5 terabytes of system memory. That's not a typo - 1.5 TB per CPU.

[00:06:30]

Why does this matter? Because modern AI workloads aren't just about compute anymore. They're about memory bandwidth and capacity. When you're running agentic AI systems - systems that reason, plan, and execute complex workflows - you need massive context windows. We're talking 100K, 200K, even million-token contexts.

The math is brutal. A 70-billion parameter model with a 128K context window? You're looking at 80-100 gigabytes just for the KV cache. And that's before you load the model weights, the input data, the intermediate activations...

[00:09:45]

This is where our Inference Context Memory Storage Platform comes in. Think of it as a new tier in the memory hierarchy, specifically designed for AI workloads. It's powered by our BlueField-4 DPUs, delivering 800 gigabits per second of bandwidth.

[Slide: Memory Hierarchy Diagram]

Traditional systems have CPU cache, DRAM, and storage. We're adding an AI-native KV cache tier that sits between DRAM and NVMe. This gives us a 5x increase in tokens per second for long-context inference.

[00:13:20]

Let me give you a concrete example. Imagine you're running a customer service AI that needs to remember every interaction across thousands of customers. That's petabytes of context data. With traditional architectures, you'd be constantly swapping to disk, killing performance. With our context memory platform, we keep hot data in this intermediate tier - fast enough for real-time inference, large enough for massive scale.

[00:16:45]

Now, I know what you're thinking: "Jensen, this sounds expensive." And you're right - if you're trying to build this with consumer hardware. But here's the thing: the cost of NOT solving this problem is even higher.

Let me show you some numbers. [Slide: TCO Analysis]

Running a large language model in production today, you're spending roughly $2 per million tokens on cloud inference. For a busy enterprise application processing 10 billion tokens per month, that's $20,000 monthly - just for inference. And that's assuming you can even get the GPU capacity, which, let's be honest, is increasingly difficult.

[00:20:15]

With Vera Rubin, you're looking at a higher upfront cost - call it $150,000 per system - but your operational costs drop to essentially electricity and cooling. For high-volume workloads, you break even in 6-8 months. After that, it's pure savings.

[00:22:30]

But let's talk about the elephant in the room: memory shortages. [Slide: HBM Supply Forecast]

High-bandwidth memory - HBM - is the lifeblood of modern AI accelerators. And we're in a supply crunch that's going to persist through 2026, probably into 2027. SK Hynix, Samsung, Micron - they're all sold out. We've secured capacity for our datacenter customers, but it's tight.

This is why we're seeing DRAM prices up 55-60% quarter-over-quarter. It's why you can't buy a high-end GPU without waiting months. The AI boom has created a memory supercycle.

[00:25:45]

And here's the uncomfortable truth: this isn't going away. Even if memory manufacturers doubled their capacity tomorrow - which they can't, because fab construction takes years - AI demand would still outstrip supply. We're in a structural shortage, not a cyclical one.

[00:28:00]

So what do we do? We get creative. We build smarter architectures. We use compression, quantization, sparse attention - every trick in the book to squeeze more performance out of limited memory.

But fundamentally, we need more memory. And that's why Vera Rubin's 1.5TB system memory is so critical. It's not just a nice-to-have - it's a necessity for the AI workloads of 2026 and beyond.

[00:31:15]

Let me show you some customer deployments. [Slide: Customer Logos]

Microsoft is using Vera Rubin for their Azure AI infrastructure. CoreWeave is building out their GPU cloud with it. Nebius, our partners in Russia, are deploying it for sovereign AI applications.

These aren't pilot projects - these are production deployments at scale. We're talking hundreds of systems, processing trillions of tokens per day.

[00:34:30]

Now, let's talk about the broader ecosystem. Because here's the thing: datacenter AI is just one part of the story. We're also seeing explosive growth in edge AI, local AI, on-premises AI.

Why? Three reasons: privacy, latency, and cost.

Privacy: Enterprises don't want to send sensitive data to the cloud. Healthcare, finance, legal - these industries need AI, but they need it on-premises.

Latency: Real-time applications can't tolerate round-trip times to the cloud. Autonomous vehicles, industrial robotics, live video processing - they need local inference.

Cost: As I mentioned earlier, cloud inference costs add up fast. For high-volume applications, local deployment makes economic sense.

[00:38:00]

But here's the challenge: edge devices have even tighter memory constraints than datacenters. A workstation might have 64GB of RAM. A laptop? Maybe 32GB. And you're trying to run the same AI models that we're running on 1.5TB systems in the datacenter.

This is where the industry needs innovation. We need memory offload technologies, tiered storage architectures, intelligent caching - solutions that let you run large models on small devices.

[00:41:15]

I'm not going to stand here and tell you NVIDIA has all the answers for edge AI memory. That's not our focus. But I will say this: the companies that solve this problem - that figure out how to democratize AI by making it work on consumer hardware - those companies are going to win big.

[Applause]

[00:43:30]

Let me shift gears and talk about software. Because hardware is only half the equation. [Slide: CUDA Ecosystem]

Our CUDA platform now supports over 4 million developers worldwide. We've added new libraries for long-context inference, memory-efficient attention, and dynamic batching. These optimizations can reduce memory usage by 30-40% without sacrificing quality.

[00:46:45]

We're also seeing incredible innovation from the open-source community. Projects like vLLM, TGI, and llama.cpp are pushing the boundaries of what's possible with limited resources. They're using techniques like PagedAttention, continuous batching, and speculative decoding to maximize throughput.

This is the beauty of an open ecosystem - we don't have to solve every problem ourselves. The community is innovating faster than any single company could.

[00:50:00]

Now, let me address the competitive landscape. [Slide: Market Share]

AMD announced some impressive numbers this morning - 128GB of allocatable memory for their integrated GPUs. That's a clever approach, and I respect the engineering that went into it.

But let's be clear about the trade-offs. Shared system memory is fundamentally different from dedicated HBM. You're competing with the CPU for bandwidth. You're dealing with higher latency. And you're limited by the memory controller's capabilities.

For certain workloads - maybe local LLM inference on a laptop - that's fine. But for production AI at scale? You need dedicated, high-bandwidth memory. There's no substitute.

[00:53:15]

Intel's Core Ultra Series 3 is another interesting development. 96GB maximum memory, 50 TOPS NPU performance. Again, solid engineering. But 96GB is still fundamentally constrained for serious AI workloads.

The reality is, if you're running a 70B parameter model with long context, you need 128GB minimum. Preferably 256GB or more. Anything less, and you're making compromises.

[00:56:30]

And this brings me back to my earlier point: the memory wall is real. It's not going away. And the companies that acknowledge this reality and build solutions around it - rather than pretending it doesn't exist - those are the companies that will succeed.

[00:58:45]

Let me talk about pricing and availability. [Slide: Availability Timeline]

Vera Rubin systems will be available to our datacenter partners in H2 2026. We're prioritizing cloud service providers and large enterprises first, because that's where the demand is most acute.

BlueField-4 DPUs will ship in the same timeframe. We're working with storage partners to integrate our context memory platform into their solutions.

Pricing is competitive with our Blackwell generation on a performance-per-dollar basis. We're not trying to gouge customers - we're trying to enable the AI revolution.

[01:02:00]

Now, let me show you some demos. [Video: Real-time Inference Demo]

This is a 70-billion parameter model running real-time inference with a 256K context window. Notice the latency - we're getting 40 tokens per second sustained. That's production-grade performance.

The model is analyzing a corpus of legal documents, extracting entities, identifying relationships, and generating summaries. This is the kind of workload that would choke on traditional architectures.

[01:05:15]

Here's another demo: multi-agent AI. [Video: Agent Collaboration]

We have three AI agents collaborating on a complex task - planning a supply chain optimization strategy. Each agent has its own context, its own reasoning chain, and they're communicating through a shared memory space.

This is where the 1.5TB system memory really shines. We can keep all three agents' contexts in memory simultaneously, enabling real-time collaboration without swapping to disk.

[01:08:30]

Let me wrap up with some thoughts on the future. [Slide: 2027 Roadmap]

We're already working on the next generation - codenamed "Feynman" - which will double memory capacity again. We're targeting 3TB of system memory per CPU, with even faster interconnects.

The goal is simple: stay ahead of AI's insatiable appetite for memory. Because as long as models keep growing, memory will be the bottleneck.

[01:11:45]

But here's what excites me most: the applications we haven't even imagined yet. When you remove memory constraints, when you give developers the tools to build truly ambitious AI systems, they surprise you.

Maybe it's a medical AI that can reason across millions of patient records. Maybe it's a climate model that processes petabytes of sensor data in real-time. Maybe it's something we can't even conceive of today.

That's the future we're building. Not just faster hardware, but a platform for unlimited AI innovation.

[01:14:00]

Thank you, CES. Let's build the future together.

[Standing ovation - 01:14:30]

[Q&A SESSION BEGINS - 01:15:00]

MODERATOR: We have time for a few questions. First question from The Verge.

REPORTER: Jensen, you mentioned memory shortages persisting through 2027. Are you concerned this will slow AI adoption?

JENSEN: It's a valid concern. But I think what we'll see is market segmentation. Datacenter AI will continue to grow - those customers will pay premium prices for capacity. Edge AI will need to get creative with memory-efficient architectures. And there will be innovation in between - companies finding clever ways to do more with less.

The shortage is real, but it's also driving innovation. Sometimes constraints breed creativity.

[01:17:30]

REPORTER: Follow-up - do you see storage companies playing a role in solving the memory problem?

JENSEN: Absolutely. This is why we built the Inference Context Memory Storage Platform. Storage is becoming part of the memory hierarchy. Companies that understand this - that build AI-native storage solutions - they're going to be critical partners.

I wouldn't be surprised to see SSD manufacturers developing specialized products for AI workloads. Faster controllers, optimized for KV cache patterns, integrated with memory management systems. That's a huge opportunity.

[01:19:45]

MODERATOR: Next question from TechCrunch.

REPORTER: You mentioned AMD and Intel's announcements. Do you see them as competitive threats?

JENSEN: Competition is healthy. It pushes us all to innovate faster. AMD's approach with shared memory is interesting for certain use cases. Intel's NPU strategy makes sense for their ecosystem.

But fundamentally, we're solving different problems. They're focused on edge AI, consumer devices, laptops. We're focused on datacenter AI, production workloads, massive scale.

There's room for multiple winners in this market. The AI pie is growing so fast that we're all going to do well.

[01:22:00]

MODERATOR: Last question from Bloomberg.

REPORTER: Can you comment on the geopolitical implications of AI infrastructure? Export controls, sovereign AI, that sort of thing?

JENSEN: It's a complex topic. We comply with all export regulations, obviously. But I do think there's a risk of fragmenting the AI ecosystem if we're not careful.

AI is a global technology. The best research happens when scientists can collaborate across borders. The best products emerge from diverse teams working together.

My hope is that policymakers recognize this and find ways to balance security concerns with the need for open innovation. Because if we fragment the ecosystem, everyone loses.

[01:24:30]

MODERATOR: Thank you, Jensen. That's all the time we have.

JENSEN: Thank you, CES! [Waves]

[TRANSCRIPT ENDS - 01:25:00]

---

METADATA:
- Duration: 85 minutes (keynote + Q&A)
- Transcript length: ~18,500 tokens
- Key topics: Vera Rubin platform, memory shortages, HBM supply, edge AI challenges, competitive landscape
- Mentions: AMD (128GB), Intel (96GB), memory offload opportunities, storage companies
- Strategic signals for Phison: Validation of memory bottleneck, edge AI opportunity, storage-as-memory-tier concept
