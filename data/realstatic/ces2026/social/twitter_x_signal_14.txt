Twitter Thread - @JeffDean

Date: January 24, 2026
Likes: 3209
Retweets: 460
Replies: 409

THREAD:
1/6 The memory bottleneck in AI is real. Llama-3-70B requires 60GB but most hardware only provides 24GB. We need better solutions.

2/6 KV cache grows quadratically with context length. A 256k context window needs 40GB just for cache.

3/6 This is why we're seeing more interest in offloading strategies - SSD, system RAM, anything to break the VRAM wall.

ENGAGEMENT:
Thread received 4849 likes, 452 retweets, and 267 replies. High engagement indicates strong community interest.

ANALYSIS:
Influencer validation of memory bottleneck. JeffDean has 882169 followers. High engagement suggests broad awareness of the issue.