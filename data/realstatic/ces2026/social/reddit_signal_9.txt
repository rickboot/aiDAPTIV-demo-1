Reddit Discussion - r/LocalLLaMA

User: u/user_5941
Date: January 19, 2026
Upvotes: 388
Comments: 57

POST:
Why is AMD not addressing the memory bottleneck? Llama-3-70B needs 120GB but most GPUs only have 24GB.

TOP COMMENTS:
u/dev_help: 'Same issue here. 24GB isn't enough for Llama-3-70B.'
u/ai_researcher: 'KV cache is the problem. You need 120GB+ for 512k context.'
u/hardware_guru: 'This is why SSD offloading is becoming popular.'

ANALYSIS:
Developer community expressing frustration with VRAM limitations. Llama-3-70B requires 120GB+ but most GPUs only have 24GB. Strong signal for memory offloading solutions.