@karpathy (Andrej Karpathy)
January 7, 2026 Â· 10:42 AM

Agentic workflows are memory-hungry beasts. 

The problem: context windows growing faster than VRAM capacity. Every agent needs to maintain state, tool outputs, reasoning chains... it adds up FAST.

We're seeing 100K+ token contexts becoming standard. That's 50-80GB of KV cache for a 70B model. 

This will be THE bottleneck of 2026. ðŸ§µ

---

@karpathy
Not just inference either. Fine-tuning with long contexts? Forget about it on consumer hardware.

Cloud solutions exist (obviously) but:
- Privacy concerns for sensitive data
- Latency for real-time apps
- Cost at scale

We need better local memory architectures.

---

@karpathy
Interesting approaches I'm watching:
1. Quantized KV cache (quality trade-offs)
2. Sparse attention (works for some use cases)
3. Tiered memory systems (SSD offload, etc.)
4. Model compression (but you lose capability)

None are perfect. We're in an awkward transition period.

---

@karpathy
The irony: We finally have models smart enough to be truly useful agents, but we can't run them locally because of memory constraints.

Cloud providers love this. Local AI advocates... not so much.

---

@karpathy
Prediction: By end of 2026, we'll see specialized hardware for context memory management. Either from GPU vendors or storage companies.

The market opportunity is too big to ignore.

---

Replies (847):

@AIResearcher42
This is why I'm skeptical of "AI PC" marketing. 16GB RAM isn't going to cut it for serious agentic workflows.

@karpathy
Exactly. The spec sheets don't match the use case requirements.

---

@HardwareEngineer
Have you looked at CXL memory expansion? Seems promising for this exact problem.

@karpathy
Interesting tech but still expensive and not widely available. Need solutions that work with today's hardware.

---

@StartupFounder
Our API bills for Claude/GPT-4 are $40K/month. Would LOVE a local solution but can't sacrifice quality.

@karpathy
This is the real tension. Cloud quality is high, but costs scale linearly with usage. Local has upfront costs but better unit economics at scale.

---

@PhisonElectronics (Official)
@karpathy We're working on SSD-based memory offload solutions for exactly this use case. Would love to discuss further. DM open.

[Note: This reply has 234 likes, 89 retweets]

---

@OpenSourceDev
The fact that a storage company is replying to this thread tells you everything about where the market is heading.

---

[Thread continues with 847 total replies]
