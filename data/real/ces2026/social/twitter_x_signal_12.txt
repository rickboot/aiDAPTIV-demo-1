Twitter Thread - @ylecun

Date: January 22, 2026
Likes: 4494
Retweets: 317
Replies: 231

THREAD:
1/3 The memory bottleneck in AI is real. Llama-3-70B requires 60GB but most hardware only provides 32GB. We need better solutions.

2/3 KV cache grows quadratically with context length. A 128k context window needs 30GB just for cache.

3/3 This is why we're seeing more interest in offloading strategies - SSD, system RAM, anything to break the VRAM wall.

ENGAGEMENT:
Thread received 4549 likes, 971 retweets, and 216 replies. High engagement indicates strong community interest.

ANALYSIS:
Influencer validation of memory bottleneck. ylecun has 290672 followers. High engagement suggests broad awareness of the issue.