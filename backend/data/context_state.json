{
  "total_tokens": 23756,
  "active_documents": [
    {
      "name": "nvidia_CES_2026_keynote_transcript.txt",
      "category": "documentation",
      "size_kb": 74.1,
      "content": "0:00\nReady, go.\n0:07\n[Music]\n0:18\n[Music]\n0:29\n[Music] Heat. Heat.\n0:42\nHeat. [Music]\n0:55\nHeat.\n1:03\nHeat.\n1:17\n[Music] Heat.\n1:27\nHeat. Heat. [Music]\n1:40\n[Music]\n1:47\n[Music]\n2:01\n[Music]\n2:12\nHeat. Heat.\n2:17\nHeat.\n2:31\n[Music] Heat.\n2:40\nHeat. Heat. [Music]\n2:55\n[Music]\n3:03\n[Music]\n3:21\n[Music]\n3:26\nWelcome to the stage, Nvidia founder and CEO, Jensen Wong.\n3:33\n[Music] [Applause]\n3:40\nHello, Las Vegas. Happy New Year.\n3:45\nWelcome to CES. Well, we have about 15 keynotes worth of\n3:51\nmaterial to pack in here. I'm so happy to see all of you. You got 3,000 people in this auditorium. There's 2,000 people\n3:58\nin a courtyard watching us. There's another thousand people apparently in the fourth floor where there were\n4:03\nsupposed to be Nvidia show floors all watching this keynote and of course millions around the world are going to\n4:08\nbe watching this to kick off this new year. Well, every 10 to 15 years the\n4:15\ncomputer industry resets. A new platform shift happens\n4:22\nfrom mainframe to PC, PC to internet, internet to cloud, cloud to mobile. Each\n4:27\ntime the world of applications target a new platform, that's why it's\n4:34\ncalled a platform shift. You write new applications for a new computer.\n4:39\nExcept this time, there are two simultaneous platform shifts in fact happening at the same\n4:45\ntime. While we now move to AI, applications\n4:51\nare now going to be built on top of AI. At first, people thought AIs are applications. And in fact, AIS are\n4:58\napplications, but you're going to build applications on top of AIS.\n5:03\nBut in addition to that, how you run the software, how you\n5:09\ndevelop the software fundamentally changed. The entire five\n5:15\nstack of the computer industry is being reinvented. You no longer program the software, you\n5:22\ntrain the software. You don't run it on CPUs, you run it on GPUs.\n5:28\nAnd whereas applications were pre-recorded, pre-ompiled\n5:34\nand run on your device, now applications understand the context and generate\n5:40\nevery single pixel, every single token completely from scratch every single time.\n5:46\nComputing has been fundamentally reshaped as a result of accelerated computing, as a result of artificial\n5:52\nintelligence. Every single layer of that five layer cake is now being re\n5:57\nreinvented. Well, what that means is some 10 trillion dollars or so of the\n6:03\nlast decade of computing is now being modernized to this new way of doing computing. What that means is hundreds\n6:11\nof billions of dollars, a couple hundred billion dollars in VC funding each year is going into modernize and inventing\n6:18\nthis new world. And what it means is a hundred trillion dollars of industry,\n6:23\nseveral percent of which is R&D budget is shifting over to artificial intelligence. People ask where is the\n6:30\nmoney coming from? That's where the money is coming from. the modernization of AI to AI, the shifting of R&D budgets\n6:39\nfrom classical methods to now artificial intelligence methods. Enormous amounts\n6:44\nof investments coming into this industry, which explains why we're so busy. And this last year was no\n6:50\ndifference. This last year was incredible. This last year, there's a slide coming.\n6:57\nThis is what happens when you don't practice. It's the first keynote of the year. I\n7:03\nhope it's your first keynote of the year. Otherwise, you can you have been pretty pretty busy. It's our first keynote of the year. We're going to get\n7:09\nthe spiderw webs out. And so 2025 was an\n7:14\nincredible year. It's just see it seems like everything was happening all at the same time and it in fact it probably\n7:20\nwas. The first thing of course is scaling loss.\n7:25\nIn 2015, the first language model that I thought\n7:31\nwas really going to make a difference made a huge difference. It was called BERT. 2017, Transformers came. It wasn't\n7:38\nuntil 5 years later, 2022, that Chad GPT moment happened and it awakened the\n7:44\nworld to the possibilities of artificial intelligence. Something very important happened a year\n7:49\nafter that. The first 01 model from chat GPT the first reasoning model completely\n7:56\nrevolutionary invented this idea called test time scaling which is very common sens common sensical thing not only do\n8:04\nwe pre-train a model to learn we postrain it with our re reinforcement learning so that it could learn skills\n8:10\nand now we also have test time scaling which is another way of saying thinking you think in real time each one of these\n8:18\nphases of artificial intelligence requires enormous amount of compute and the computing law continued to scale.\n8:24\nLarge language models continued to get better. Meanwhile, another breakthrough happened and this breakthrough happened\n8:31\nin 2024. Agentic systems starting to emerge in 2025. It started to pervase to to uh\n8:39\nproliferate just about everywhere. Agentic models that have the ability to reason,\n8:45\nlook up information, do research, use tools, plan futures, simulate outcomes.\n8:53\nAll of a sudden started to solve very, very important problems. One of my favorite Agentic models is called\n9:00\ncursor, which revolutionized the way we do software programming at NVIDIA. Agentic systems are going to really take\n9:06\noff from here. Of course, there were other types of AI. We know that large language models isn't the only type of\n9:12\ninformation. Wherever the universe has information, wherever the universe has structure, we could teach a large\n9:19\nlanguage model a form of language model to go understand that information to\n9:26\nunderstand its representation and to turn that into an AI. One of the biggest\n9:31\nmost important one is physical AI. AIs that understand the laws of nature. And\n9:37\nthen of course physical AI is about AI interacting with the world. But the\n9:42\nworld itself has information encoded information and that's called AI physics. AI that in the case of physical\n9:50\nAI you have AI that interacts with the physical world and you have AI physics. AI that understands the laws of physics.\n9:57\nAnd then lastly one of the most important things that happened last year the advancement of open models. We can\n10:05\nnow know that AI is going to proliferate everywhere when open source when open\n10:10\ninnovation when innovation across every single company and every industry around the world is activated. At the same\n10:16\ntime, open models really took off last year. In fact, last year we saw the\n10:25\nadvance of Deepseek R1, the first open model that's a reasoning system. It\n10:32\ncaught the world by surprise and it activated literally this entire\n10:38\nmovement. Really, really exciting work. We're so happy with it. Now we have\n10:44\nopening open model systems all over the world of all different kinds and we now know that open models have also reached\n10:51\nthe frontier. still solidly is six months behind the frontier models but\n10:56\nevery single six months a new model is emerging and these models are getting\n11:01\nsmarter and smarter because of that you could see the number of downloads has\n11:07\nexploded the number of downloads is growing so fast because startups want to\n11:13\nparticipate in the AI revolution large companies want to researchers want to students want to just about every single\n11:20\ncountry wants How is it possible that intelligence the digital form of intelligence will leave\n11:26\nanyone behind and so open models has really revolutionized\n11:32\nartificial intelligence last year. This entire industry is going to be reshaped as a result of that. Now we had this\n11:38\ninkling some time ago. You might have heard that several years ago we started\n11:44\nto build and operate our own AI supercomputers. We call them DGX clouds. A lot of people asked, are you going to\n11:51\nin going into the cloud business? The answer is no. We're building these DGX supercomputers for our own use. Well, it\n11:58\nturns out we have billions of dollars of supercomputers in operation so that we\n12:04\ncould develop our open models. I am so pleased with the work that we're doing.\n12:09\nIt is starting to attract attention all over the world and all over the industries because we are doing frontier\n12:15\nAI model work in so many different domains. The work that we did in proteins in digital biology. La protina\n12:23\nto be able to synthesize and generate proteins. Open fold 3 to understand the\n12:28\nunderstand the structure of proteins. EVO 2 how to understand and generate\n12:35\nmultiple proteins otherwise the beginnings of cellular cellular representation. Earth 2 AI that\n12:42\nunderstands laws of physics. The work that we did with forecast net, the work that we did with Cordiff really\n12:48\nrevolutionized the way that people are doing weather prediction. Neotron,\n12:54\nwe've now doing groundbreaking work there. The first hybrid transformer SSM\n12:59\nmodel that's incredibly fast can and therefore can think for a very long time\n13:05\nor can think very quickly with that for not a very long time and produce very very smart intelligent answers.\n13:11\nNeimotron 3 is groundbreaking work and you can expect us to deliver other versions of Neimotron 3 in the near\n13:17\nfuture. Cosmos a frontier open world foundation model\n13:24\none that understand how the world works. Groot a humanoid robotic system\n13:29\narticulation mobility locomotion. These models, these technologies are now being\n13:36\nintegrated and in the each one of these cases open to the world. Frontier human\n13:41\nor robotics models open to the world. And then today we're going to talk a little bit about Alpa Mayo, the work\n13:46\nthat we've been doing in self-driving cars. Not only do we open source the models, we also open source the data\n13:54\nthat we use to train those models because that in that way only in that\n13:59\nway can you truly trust how the models came to be. We open source all the\n14:04\nmodels. We help you make derivatives from them. We have a whole suite of libraries we call the Nemo libraries,\n14:11\nphysics li physics nemo libraries and the claim libraries. Each biono\n14:16\nlibraries. Each one of these libraries are life cycle management systems of AIS so that you could process the data, you\n14:23\ncould generate data, you could train the model, you could create the model, evaluate the model, guardrail the model\n14:28\nall the way to deploying the model. Each one of these libraries are incredibly complex and all of it is open sourced.\n14:35\nAnd so now on top of this platform, NVIDIA is a frontier AI model builder\n14:42\nand we build it in a very special way. We build it completely in the open so\n14:47\nthat we can enable every company, every industry, every country to be part of\n14:52\nthis AI revolution. I'm incredibly proud of the work that we're doing there. In fact, if you notice the tren the charts,\n14:59\nthe chart shows that our contribution to this industry is bar none. And you're\n15:05\ngoing to see us in fact continue to do that if not accelerate. These models are also world class.\n15:14\nAll systems are down. This never happens in Santa Clara.\n15:22\nIs it because of Las Vegas?\n15:30\nSomebody must have went won a jackpot outside. All systems are down.\n15:40\nOkay. I think my system's still down, but that's okay. I I I've I uh I'll make\n15:45\nit up as I go. And so so uh not only are these models uh frontier capable, not\n15:52\nonly are they open, they're also top the leaderboards. This is an area where we're very proud. They top leaderboards\n15:58\nin intelligence. Uh we have uh uh important models that understand multimodality documents otherwise known\n16:05\nas PDFs. The most valuable content in the world are captured in PDFs. But\n16:10\nthere it takes artificial intelligence to find out what's inside, interpret what's inside and help you read it. And\n16:17\nso our PDF retrievers, our PDF parsers are worldclass. Our speech recognition models absolutely\n16:25\nworldclass. Our retrieval models, basically search, semantic search, AI\n16:30\nsearch, the database engine of the modern AI era, world class. So we're on\n16:36\ntop of leaderboards constantly. This is an area we're very proud of. And all of that is in service of your ability to\n16:45\nbuild AI agents. This is really a groundbreaking area of development. You\n16:50\nknow, at first when PE when Chad GPT came out, people said, you know, uh gosh, it it produced really interesting\n16:56\nresults, but it hallucinated greatly. And the reason why it hallucinated, of course, it could memorize everything um\n17:03\nin the past, but it can't memorize everything in the future, in the current. And so, it needs to be grounded\n17:08\nin research. It has to do fundamental research before it answers a question. The ability to reason about do I have to\n17:15\ndo research? Do I have to use tools? How do I break up a problem into steps? Each one of these steps something that that\n17:21\nthe AI model knows how to do and together it is able to compose it into a\n17:27\nsequence of steps to perform something it's never done before, never been trained to do. This is the wonderful\n17:33\ncapability of reasoning. We could we could be we can encounter a circumstance we've never seen before and break it\n17:40\ndown into circumstances and knowledge or rules that we know how to do because\n17:46\nwe've experienced it in the past. And so the ability for AI models now to be able to reason incredibly powerful. The\n17:53\nreasoning capability of agents open the doors to all of these different applications. We no longer have to train\n17:58\nan AI model to know everything on day one just as we don't have to know\n18:04\neverything on day one. That we should be able to in every circumstance reason about how to solve that problem. Large\n18:11\nlanguage models has now made this fundamental leap. The ability to use reinforcement learning and chain of\n18:17\nthought and you know search and planning and all these different techniques and reinforcement learning has made it\n18:22\npossible for us to have this basic capability and is also now completely\n18:27\nopen sourced. But the thing that's really terrific is another breakthrough that happened and the first time I saw\n18:33\nit was with Arvin's perplexity. Perplexity, the search company, the AI\n18:39\nsearch company, really f really innovative company. And the first time I realized they were using multiple models\n18:45\nat the same time, I thought it was completely genius. Of course, we would do that. Of course, an AI would also\n18:52\ncall upon all of the world's great AIs to solve the problem it wants to solve at any part of the reasoning chain. And\n19:00\nthis is the reason why AIs are really multi-modal\n19:06\nmeaning they understand speech and images and text and videos and 3D\n19:13\ngraphics and proteins. It's multimodal. It's also multi-model\n19:18\nmeaning that it should be able to use any model that best fits the task. It is\n19:25\nmulticloud by definition. care for because these AI models are sitting in all these different places and it also\n19:32\nis hybrid cloud because if you're an enterprise company or you've built a robot or whatever that device is\n19:39\nsometimes it's at the edge sometimes a radio cell tower maybe sometimes it's in\n19:44\nan enterprise or maybe it's a place where a hospital where you need to have the the data in real time right next to\n19:51\nyou whatever those applications are we know now this is what an AI application\n19:57\nlooks like in the future. Or another way to think about that because future\n20:02\napplications are built on AIs. This is the basic framework of future\n20:08\napplications. This basic framework, this basic structure of agentic AIs that could do\n20:15\nthe things that I'm talking about that is multi-model has now turbocharged\n20:21\nAI startups of all kinds. And now you can also because of the all of the open\n20:27\nmodels and all the tools that we provided you, you could also customize your AIS to teach your AI skills that\n20:34\nnobody else is teaching. Nobody else is causing their AI to become intelligent or smart in that way. You could do it\n20:42\nfor yourself. And that's the work that we do with Neimotron Nemo and all of the things that we do with open models is\n20:48\nintended to do. You put a smart router in front of it and that router is essentially a manager that decides which\n20:55\none of the task based on the intention of the prompts that you give it which one of the models is best fit for that\n21:02\napplication for that solving that problem. Okay. So now when you think about this architecture, what do you\n21:09\nhave? When you think about this architecture, all of a sudden you have an AI that's on the one hand completely\n21:16\ncustomizable by you. Something that you could teach to do your own very skills\n21:21\nfor your company, something that's domain secret, something where you have deep domain expertise. Maybe you've got\n21:28\nall of the data that you need to train that AI model. On the other hand, your\n21:33\nAI is always at the frontier by definition. You're always at the\n21:39\nfrontier on the one hand. You're always customized on the other hand and it should just run. And so we thought we\n21:45\nwould make the simplest of examples to make it available to you. This entire framework we call a blueprint and we\n21:53\nhave blueprints that are integrated into enterprise SAS platforms all over the world and we're really pleased with the\n21:59\nprogress. But what we do is show you a a short example of something that anybody can do.\n22:07\nLet's build a personal assistant. I wanted to help me with my calendar, emails, to-do lists, and even keep an\n22:13\neye on my home. I use Brev to turn my DGX Spark into a personal cloud. So, I\n22:20\ncan use the same interface whether I'm using a cloud GPU or a DGX Spark. I use a Frontier model API to easily get\n22:26\nstarted. [Music] I want it to help me with my emails. So,\n22:32\nI create an email tool for my agent to call. I want my emails to stay private, so\n22:38\nI'll add an open model that's running locally on the Spark.\n22:44\nNow, for any job, I want the agent to use the right model for the right task. So, I'll use an intentbased model\n22:50\nrouter. This way, prompts that need email will stay on my Spark, and everything else\n22:57\ncan call the Frontier model. I want my assistant to interact with my world, so I'll hook it up to Hugging Faces Reichi\n23:04\nMini Robot. My agent controls the head, ears, and camera of the Reachi with tool calls. I\n23:11\nwant to give Richi a voice, and I really like 11 Labs, so I'll hook up their API. Hi, I'm Richi running on DGX Spark.\n23:18\nHey Richi, what's on my to-do list today? your to-do list today.\n23:24\nGrab groceries, eggs, milk, butter, and send Jensen the new script.\n23:30\nOkay, let's send Jensen that update. Tell him we'll have it for him by the end of the day. We'll do.\n23:35\nReachi, there's a sketch, too. Can you turn it into an architectural rendering?\n23:40\nSure. [Music]\n23:49\nNice. Now make a video and show me around the room.\n23:54\nHere you go. That's great. With Brev, I can share access to my\n24:00\nSpark and Reachi, so I'm going to share it with Anna. Hey Richi, what's Potato up to?\n24:08\nHe's on the couch. I remember you don't like this. I'll tell him to get off. Potato, off the\n24:15\ncouch. With all the progress in open source, it's incredible to see what you can build. I'd love to see what you create.\n24:23\n[Music] Isn't that incredible?\n24:29\nNow, the amazing thing is that is utterly trivial now. That is utterly\n24:35\ntrivial now. And yet, just a couple years ago, all of that would have been\n24:40\nimpossible. Absolutely unimaginable. Well, this basic framework, this basic way of building applications using\n24:48\nlanguage models\n24:57\nusing language models. Using language models using language\n25:04\nmodels that are pre-trained and they're proprietary, they're frontier. combine\n25:10\nit with customized language models into a aentic framework, a reasoning\n25:15\nframework that allows you to access tools and files and maybe even connect\n25:20\nto other agents. This is basically the architecture of AI\n25:27\napplications or applications in the modern age and the ability for us to create these applications are incredibly\n25:33\nfast. And notice if you give it this application um\n25:39\ninformation that it's never seen before or in a structure that has is not\n25:44\nrepresented exactly as you thought it can still reason through it and make its best\n25:51\neffort to reason through the data the information to try to understand how to solve the problem artificial\n25:57\nintelligence. Okay. So this basic framework is now being integrated and and everything that I just described. We\n26:03\nhave the benefit of working with some of the world's leading enterprise platform companies. Uh Palunteer for example\n26:10\num their their entire AI and data processing platform is being integrated\n26:15\naccelerated by Nvidia today. Service Now the world's leading customer service and\n26:21\num employee service platform. Snowflake the world's top data platform in the\n26:26\ncloud. uh incredible work that that uh is being done there. Uh code rabbit or\n26:32\nwe're using code rabbit all over Nvidia. Uh Crowdstrike creating AIs to detect to\n26:37\nfind AI threats. Uh NetApp their AI their data platform now has NVIDIA\n26:43\nsemantic AI on top of it and agentic systems on top of it uh uh to for uh for\n26:49\nthem to do customer service. But the important thing is this. Not only is this the way that you develop\n26:54\napplications now, this is going to be the user interface of your platform. So\n27:00\nwhether it's Palanteer or Service Now or Snowflake and many other companies that\n27:05\nwe're working with, the agentic system is the interface. It's no longer Excel\n27:11\nwith a bunch of, you know, squares that you enter enter information into. Maybe it's no longer could just command line.\n27:18\nthe any all of that multimodality information is now possible and the way\n27:23\nyou interact with your platform is much more well if you will simple like you're\n27:29\ninteracting with people and so that's enterprise AI being revolutionized by\n27:35\nangentic systems the next thing is physical AI this is an area that you've seen me talk about for several years in\n27:41\nfact we've been working on this for eight years the question is how do you take something that is intelligent\n27:48\ninside a computer and interacts with you with screens and speakers to something\n27:56\nthat can interact with the world. Meaning it can understand the common sense of how the world works. Object\n28:03\npermanence. If I look away and I look back, that object is still there. Um causality. If I push it, it tips over.\n28:10\nIt understands friction and gravity. It understands inertia. that a heavy truck\n28:16\nrolling down the road is going to need a little bit more time to stop, that a\n28:21\nball is going to keep on rolling. These ideas are common sense to even a\n28:26\nlittle child, but for AI, it's completely unknown. And so we have to\n28:32\ncreate a system that allows AIs to learn the the common sense of the physical world, learn its laws, but also to be\n28:42\nable to of course learn from data and the data is quite scarce and to be able to evaluate whether that AI is working,\n28:50\nmeaning it has to simulate in an environment. How does an AI know that\n28:55\nthe the actions that it's performing is consistent with what it should do if it doesn't have the ability to simulate the\n29:02\nresponse of the physical world back on its actions. The response of its actions is really important to simulate\n29:08\notherwise there's no way to evaluate it. It's different every time. And so this basic system requires three computers.\n29:16\nOne computer, of course, the one that we know that Nvidia builds for training the AI models. Another computer that we know\n29:24\nis to inference the computer. Inference the models. Inferencing the model is essentially a robotics computer that\n29:31\nruns in a car or runs in a robot or runs in a factory runs anywhere at the edge. But there has to be another computer\n29:37\nthat's designed for simulation. And simulation is at the heart of almost everything Nvidia does. This is this is\n29:44\nwhere we are most comfortable. and simulation was really the foundations of\n29:50\nalmost everything that we've done with physical AI. So we have three computers and multiple stacks that run on these\n29:57\ncomputers, these libraries to make them useful. Omniverse is our digital twin physically based simulation world.\n30:04\nCosmos, as I mentioned earlier, is our foundation model, not a foundation model for language, but a foundation model of\n30:11\nthe world and is also aligned with language. You could say something like,\n30:17\nyou know, what's happening to the ball and they'll they'll tell you the ball's rolling down the street. And so a world\n30:22\nfoundation model and then of course the robotics models. We have two of them. One of them is called Groot. The other\n30:29\none's called Alpamo that I'm going to tell you about. Now the one of the most important things that we have to do with physical AI is to create the data to\n30:37\ntrain the AI in the first place. Where does that data come from? Rather than instead of having languages because we\n30:43\ncreated a bunch of text that are what we consider ground truth that the AI can\n30:49\nlearn from, how do we teach an AI the ground truth of physics? There lots and lots of videos, lots and lots of videos,\n30:56\nbut hardly enough to capture the diversity and the type of interactions that we need. And so this is where great\n31:04\nminds came together and transformed what used to be compute into data. Now\n31:13\nusing synthetic data generation that is grounded and conditioned by the laws of\n31:19\nphysics, grounded and conditioned by ground truth, we can now selectively\n31:25\ncleverly generate data that we can then use to train the AI. So for example,\n31:31\nwhat comes into this AI, this Cosmos AI world model on the left on over here is\n31:38\nthe output of a traffic simulator. Now this traffic simulator\n31:44\nis hardly enough for an AI to learn from. We can take this, put it into a\n31:50\nCosmos foundation model and generate surround video that is physically based\n31:57\nand physically plausible that the AI can now learn from. And there are so many\n32:02\nexamples of this. Let me show you what Cosmos can do.\n32:08\nThe chat GPT moment for physical AI is nearly here, but the challenge is clear.\n32:15\nThe physical world is diverse and unpredictable. Collecting real world training data is\n32:22\nslow and costly and it's never enough. The answer is synthetic data. It starts\n32:29\nwith NVIDIA Cosmos, an open Frontier World Foundation model for physical AI\n32:37\npre-trained on internet scale video, real driving and robotics data, and 3D\n32:43\nsimulation. Cosmos learned a unified representation of the world, able to align language,\n32:50\nimages, 3D, and action. It performs physical AI skills like\n32:57\ngeneration, reasoning, and trajectory prediction from a single image. Cosmos generates\n33:04\nrealistic video from 3D scene descriptions, physically\n33:11\ncoherent motion, from driving telemetry and sensor logs,\n33:17\nsurround video from planning simulators, multi- camera\n33:23\nenvironments, or from scenario prompts. It brings edge\n33:29\ncases to life. Developers can run interactive closed loop simulations in Cosmos. When actions\n33:37\nare made, the world responds. Cosmos reasons.\n33:44\nIt analyzes edge scenarios, breaks them down into familiar physical interactions, and reasons about what\n33:51\ncould happen next. Cosmos turns compute into data, training\n33:57\nAVs for the longtail and robots how to adapt for every scenario.\n34:05\n[Music]\n34:11\nI know it's incredible. Cosmos is the world's leading foundation\n34:18\nmodel. World foundation model. It's been downloaded millions of times, used all over the world, getting world getting\n34:25\nthe world ready for this new era of physical AI. We use it ourselves as well. We use it ourselves to create our\n34:33\nself-driving car. using it for scenario generation and\n34:38\nusing it for evaluation. We could have something that allows us to effectively travel billions,\n34:46\ntrillions of miles, but doing it inside a computer. And we've made enormous\n34:52\nprogress. Today, we're announcing Alpio, the world's first\n34:58\nthinking reasoning autonomous vehicle AI. Up myio\n35:05\nis trained end to end. Literally from camera in to actuation out. The camera\n35:12\nin lots and lots of miles that are driven by itself or human drive it driven using human\n35:20\ndemonstration and we have lots and lots of miles that are generated by cosmos. In addition to\n35:26\nthat, hundreds of thousands of examples are labeled very very carefully so that\n35:33\nwe could teach the car how to drive. Alpha Mayo does something that's really special. Not only does it take sensor\n35:39\ninput and activates steering wheel, brakes and and acceleration, it also\n35:47\nreasons about what action it is about to take. It tells you what action it's\n35:53\ngoing to take. the reasons by which it came about that action and then of course the trajectory.\n36:00\nAll of these are coupled directly and trained very specifically by a large\n36:05\ncombination of human trained and as well as Cosmos generated data. The result of\n36:12\nit is just really incredible. Not only does your car drive as you would expect it to drive and it drives so naturally\n36:19\nbecause it learned directly from human demonstrators but in every single scenario when it comes up to the\n36:25\nscenario it reasons about it tells you what it's going to do and it reasons about what you what's about to do. Now\n36:31\nthe reason why this is so important is because of the long tale of driving there. It's impossible for us to simply\n36:39\ncollect every single possible scenario for everything that could ever happen in every single country and every single\n36:45\ncircumstance that's possibly ever going to happen for all of population.\n36:50\nHowever, it is very unlikely it's very likely that every scenario if decomposed\n36:57\ninto a whole bunch of other smaller scenarios are quite normal for you to understand. And so these long tales will\n37:05\nbe decomposed into quite normal circumstances that the car knows how to deal with. It just needs to reason about\n37:11\nit. And so let's take a look. Everything you're about to see is one shot. It's uh no hands\n37:19\n[Music] routing to your destination. Buckle up.\n37:29\n[Music]\n37:40\nHeat. Heat. [Music]\n38:13\nHeat. Heat. [Music]\n38:37\nHeat. Heat.\n38:45\n[Music]\n38:57\n[Applause] [Music]\n39:09\nHeat. Heat. [Music]\n39:26\nHeat. Heat. [Music] [Applause]\n39:32\n[Music] [Applause]\n39:39\n[Music]\n39:49\n[Music]\n39:56\nYou have arrived.\n40:12\nWe started working on self-driving cars eight years ago. And the reason for that is because we reasoned early on that\n40:19\ndeep learning and artificial intelligence was going to reinvent the entire computing stack. And if we were\n40:24\never going to understand how to navigate ourselves and how to guide the industry\n40:30\ntowards this new future, we have to get good at building the entire stack. Well,\n40:36\nas I mentioned earlier, AI is a five layer cake. The lowest layer is land\n40:41\npower and shell. In the case of robotics, the lowest layer is the car. The next layer above it is chips, GPUs,\n40:48\nnetworking chips, CPUs, all that kind of stuff. The next layer above that is the\n40:54\ninfrastructure. That infrastructure in this particular case as I mentioned with physical AI is\n41:00\nomniverse and cosmos. And then above that are the models. And\n41:06\nin the case of the models above that I just shown you.\n41:12\nThe model here is called Alpha Mayo. And Alpha Mayo today is open sourced. We\n41:18\nthis incredible body of work. It took several thousand people. Our AV team is\n41:24\nseveral thousand people. Just to put in perspective, our partner uh Ola, I think\n41:30\nOla's here in the audience somewhere. Uh Mercedes agreed to partner with us five\n41:35\nyears ago to go make all of this possible. We imagine that someday a\n41:41\nbillion cars on a road will all be autonomous. You could either have it be a robo taxi that you're you're\n41:47\nyou'rechestrating and and renting from somebody or you could own it and is driving for driving\n41:52\nby itself or you could decide to drive for yourself. And so but every single car will have autonomous vehicle\n41:58\ncapability. Every single car will be AI powered. And so the the the model layer in this case is Alpha Mayo and the\n42:05\napplication above that is the Mercedes-Benz. Okay. And so, so this entire stack is\n42:12\nour first Nvidia first entire stack endeavor and we've been working on it for this entire time. And I'm just so\n42:18\nhappy that the first AV car from Nvidia is going to be on the road in Q1 and\n42:25\nthen it goes Europe in Q2 here in United States in Q1 then Europe in Q2 and I think it's Asia in Q3 and Q4. And the\n42:33\npowerful thing is that we're going to keep on updating it with next ver next versions of Alpameo and versions after\n42:39\nthat. There's no question in my mind now that this is going to be one of the largest robotics industries. And I'm so\n42:46\nhappy that we worked on it. And it taught us enormous amount about how to help the rest of the world build robotic\n42:52\nsystems. that deep understanding and knowing how to build it ourselves, building the entire infrastructure\n42:58\nourselves and knowing what kind of chips a robotic system would would need. In this partic particular case, dual Orins,\n43:06\nthe next generation dual Thors. These processors are designed for robotic\n43:11\nsystems and was designed for the sa highest level of safety capability. This\n43:17\ncar just got rated. It just went to production. The Mercedes-Benz CLA was\n43:25\njust rated by NCAAP, the world's safest car.\n43:34\nIt is the only system that I know that has every single line of code, the chip,\n43:40\nthe system, every line of code safety certified. The entire model system is\n43:46\nbased on a sensors are diverse and redundant and so is the self-driving car\n43:51\nstack. The Alpha Mayo stack is trained end to end and has incredible skills.\n43:57\nHowever, nobody knows until you drive it forever that it's going to be perfectly\n44:02\nsafe. And so that we the way we guard rail that is with another software stack, an entire AV stack underneath.\n44:10\nThat entire AV stack is built to be fully traceable and it's taken us some five years to build that some six seven\n44:17\nyears actually to build that second stack. These two software stacks are mirroring each other and then we have a\n44:24\npolicy and safety evaluator decide is this something that I'm very confident and can reason about driving very\n44:31\nsafely. If so, I'm going to have Alpamo do it. If it's a circumstance that I'm not very confident in and the safety um\n44:37\npolicy evaluator decide that we're going to go back to a a very a simpler safer\n44:43\nguardrail system, then it goes back to the classical AV stack where the only car in the world with both of these AV\n44:48\nstacks running and all safety systems should have diversity and redundancy. Well, our vision is that someday every\n44:55\nsingle car, every single truck will be autonomous and we've been working towards that future. This entire stack\n45:01\nis vertically integrated. Of course, in the case of Mercedes-Benz, we built the entire stack together. We're going to\n45:06\ndeploy the car. We're going to operate the stack. We're going to maintain the stack for as long as we shall live. However, like everything else we do as a\n45:14\ncompany. We build the entire stack, but the entire stack is open for the\n45:19\necosystem. And these the ecosystem working with us to build L4 and robo\n45:24\ntaxis is expanding and it's going everywhere. I fully expect this to be well this is\n45:31\nalready a giant business for us. It's a giant business for us because they use it for training our training data\n45:36\nprocessing data and training their models. They use it for synthetic data generation in some cases and some car\n45:42\nand some companies they pretty much just build uh the computers the chips that are inside the car and some companies\n45:49\nwork with us full stack some companies work with us some partial part of that. Okay. So it doesn't matter uh how much\n45:56\nyou decide to use. You know, my only request is that use a little bit of video wherever you can and uh you know,\n46:02\nbut uh the entire thing is open. Now, this is going to be the first large\n46:09\nscale mainstream um AI physical AI market and this is now\n46:15\nI think we can all agree fully here and this inflection point of going from not\n46:21\nautonomous vehicles to autonomous vehicles is probably happening right about this time. in in the next 10\n46:28\nyears, I'm fairly certain a very very large percentage of the world's cars will be autonomous or highly autonomous.\n46:35\nBut this is this basic technique that I just described in using the three computers using synthetic data\n46:41\ngeneration and simulation applies to every form of robotic systems. It could\n46:46\nbe a robot that is just an articulator, a manipulator, maybe it's a mobile robot, maybe it's a fully humanoid\n46:53\nrobot. And so the next journey, the next era for robotic systems is\n47:00\ngoing to be, you know, robots. And these robots are going to come in all kinds of different sizes. And and uh I invited\n47:06\nsome friends. Did they come?\n47:14\nHey guys, hurry up. I got a lot of stuff to cover.\n47:20\nCome on, hurry. Did you tell R2-D2 you're going to be\n47:27\nhere? Did you? And C3PO.\n47:34\nOkay. All right. Come here. Before now, one of the things that one of the things that's really You have Jetsons. They\n47:40\nhave little Jetson computers inside them. They're trained inside Omniverse.\n47:46\nAnd how about this? Let's show everybody the simulator that you were that you guys learned how to how to be robots in.\n47:53\nYou you guys want to look at that? Okay, let's look at that. Run it, please.\n48:00\n[Music]\n48:08\n[Music]\n48:32\nWhat are you doing? [Music]\n48:58\nIsn't that amazing? That's how you learn to be a robot. You\n49:05\ndid it all inside Omniverse. And the robot simulator is called Isaac. Isaac\n49:11\nSim and Isaac Lab. And anybody who wants to build a robot, you know, nobody could nobody's gonna be as cute as you. But\n49:19\nnow we have all Look at all these look at all these friends that we have building robots. We have We're building\n49:24\nbig ones. No, like I said, nobody's as cute as you guys are. But we have Neurobot and we have we have Aubot.\n49:31\nAubot over there, you know. We have uh uh LG over here. They just announced a\n49:36\nnew robot. Caterpillar. They've got the largest robots ever. That one delivers food to\n49:42\nyour house. That's connected to Uber Eats. And that's Surf Robot. I love those guys. Agility. Boston Dynamics.\n49:51\nIncredible. You got surgical robots. You got manipulated robots from Franka.\n49:57\nYou got universal robotics robot. Incredible number of different robots. And so this is the next chapter. We're\n50:04\ngoing to talk a lot more about robotics in the future, but it's not just about the robots in the end. I know\n50:10\neverything's about you guys. It's about getting there. And one of the air, one of the most important industries in the\n50:15\nworld that will be revolutionized by physical AI and AI physics\n50:21\nis the industry that started all of us at NVIDIA. It wouldn't be possible if\n50:27\nnot for the companies that I'm about to talk to. And I'm so happy that all of them starting with Cadence is going to\n50:34\naccelerate everything. Cadence CUDA X integrated into all of their simulations and solvers. They've got uh NVIDIA\n50:41\nphysical physical AIs that they're going to use for uh for different um uh physical plants and plant simulations.\n50:47\nYou got AI physics being integrated into these systems. So whether it's an EDA or\n50:53\nSDA um and in the future robotic systems, we're going to have basically\n50:58\nthe same technology that made you guys possible now completely revolutionize\n51:03\nthese design stacks. Synopsis without Synopsis, you know, Synopsis and Cadence are completely completely indispensable\n51:12\nin the world of chip design. uh synopsis is uh leads in uh in uh logic design and\n51:18\nand IP. Uh in the case of cadence, they lead physical design, the place and\n51:24\nroute uh and emulation and verification. Cadence is incredible at emulation and verification. Both of them are moving\n51:31\ninto the world of system design and system simulation. And so in the future,\n51:37\nwe're going to design your chips inside Cadence and inside Synopsis. We're going to design your systems and emulate the\n51:44\nwhole thing and simulate everything inside these tools. That's your future. We're going to give Yeah. You're going\n51:50\nto be born inside these inside these platforms. Pretty amazing, right? And so\n51:55\nwe're so happy that we're working with these these industries just as we've integrated NVIDIA into Palunteer and\n52:01\nService Now. We're integrating Nvidia into the most computationally intensive\n52:07\nsimulation industries, Synopsis and Cadence. And today we're announcing that\n52:12\nSeammens is also doing the same thing. We're going to integrate CUDA X, physical AI, Agentic AI, Nemo, Neotron\n52:20\ndeeply integrated into the world of Seammens. And the reason for that is this. First, we designed the chips and\n52:29\nall of it in the future will be accelerated by Nvidia. You're going to be very happy about that. We're going to have agent chip designers and system\n52:37\ndesigners working with us, helping us do design just as we have agentic software\n52:42\nengineers helping our software engineers code today. And so we'll have agentic chip designers and system designers.\n52:49\nWe're going to create you inside this. But then we have to build you. We have\n52:54\nto build the plants, the factories that make manufacture you. We have to design\n53:01\nthe manufacturing lines that assemble all of you. And these manufacturing plants are going to be essentially\n53:08\ngigantic robots. Incredible, isn't that right? I know. I know. And so, you're going to\n53:16\nbe designed in a computer. You're going to be made in a computer. You're going to be tested and evaluated in a computer\n53:23\nlong before long before you have to spend any time dealing gravity.\n53:29\nI know. Do you know how to deal with gravity? Can you jump?\n53:36\nCan you jump?\n53:48\nOkay. All right. Don't show off. Okay. So, so this so now\n53:54\nthe industry the industry that made Nvidia possible. We're I'm just so happy\n53:59\nthat that now the technology that we're creating is at a level of sophistication and capability that we can now help them\n54:06\nrevolutionize their industry. And so what started with with uh with them, we\n54:11\nnow have the opportunity to go back and and help them revolutionize theirs. Let's take a look at the stuff that we're going to do with Seammens.\n54:18\nCome on. Breakthroughs in physical AI are letting AI move from screens to our physical\n54:27\nworld. And just in time, as the world builds factories of every kind for chips,\n54:34\ncomputers, life-saving drugs, and AI, as the global labor shortage worsens, we\n54:40\nneed automation powered by physical AI and robotics more than ever.\n54:48\nThis where AI meets the world's largest physical industries is the foundation of NVIDIA and Seammen's partnership. For\n54:55\nnearly two centuries, Seammens has built the world's industries and now it is reinventing it for the age\n55:03\nof AI. Semens is integrating NVIDIA CUDA X\n55:08\nlibraries, AI models, and Omniverse into its portfolio of EDA,\n55:17\nCAE, and digital twin tools and platforms.\n55:23\nTogether, we're bringing physical AI to the full industrial life cycle.\n55:29\nFrom design and simulation to production\n55:35\n[Music] and operations, we stand at the beginning of a new\n55:40\nindustrial revolution, the age of physical AI built by Nvidia and Seammens\n55:46\nfor the next age of industries.\n55:52\nIncredible, right guys? What do you think? All right, I'll hang\n55:58\non tight. Just hang on tight. And so, so this is, you know, if you look at look at the world's models, there's no\n56:06\nquestion OpenAI is the the the leading token generator today. More to more open\n56:12\nAAI tokens are generated than just about anything else. The second largest group, the second largest probably open models.\n56:19\nAnd my guess is that over time because there are so many companies, so many researchers, so many different types of\n56:24\ndomains and modalities that open- source models will be by far the largest. Let's\n56:30\ntalk about somebody really special. You guys want to do that? Let's talk about Vera Rubin. Vera Rubin. Yeah, go ahead.\n56:38\nShe's a American astronomer. She was the first to observe. She\n56:43\nnoticed that the tails of the galaxies were moving about as fast\n56:50\nas the center of the galaxies. Well, I know it makes no sense. It makes no\n56:55\nsense. Newtonian physics would say just like the solar system, the planets further away from the sun is circulating\n57:03\ncirc cir circling the sun slower than the planets closer to the sun. And\n57:09\ntherefore, it makes no sense that this happens unless there's\n57:14\ninvisible bodies, we call them, she discovered, dark body, dark matter, um\n57:19\nthat occupy space even though we don't see it. And so Vera Rubin is the person that\n57:25\nwe named our next computer after. Isn't that a good idea?\n57:32\nI know. [Applause] Okay. Vera Rubin is designed to address\n57:38\nthis fundamental challenge that we have. The amount of computation necessary for AI is skyrocketing. The demand for\n57:45\nNVIDIA GPUs is skyrocketing. It's skyrocketing because models are increasing by a factor of 10, an order\n57:52\nof a magnitude every single year. And not to mention, as I mentioned, 01's\n57:58\nintroduction was an inflection point for AI. Instead of a oneshot answer,\n58:04\ninference is now a thinking process. And in order to teach the AI how to think,\n58:10\nreinforcement learning and very significant computation was introduced\n58:16\ninto post-training. It wasn't no long it's no longer supervised fine-tuning or\n58:21\notherwise known as imitation learning or supervision training. You now have\n58:26\nreinforcement learning. Essentially the computer trial trying different iterations itself learning how to\n58:33\nperform a task. The amount of computation for pre pre-training for post-training for test time scaling has\n58:40\nexploded as a result of that. And now every single inference that we do\n58:45\ninstead of just one shot the number of tokens you could just see the AI think which we appreciate. The longer it\n58:51\nthinks oftentimes it produces a better answer. And so test time scaling causes the number of tokens to be generated to\n58:57\nincrease by 5x every single year. Not to mention, meanwhile, the race is on for\n59:05\nAI. Everybody's trying to get to the next level, everybody's trying to get to the next frontier. And every time they\n59:10\nget to the next frontier, the last generation AI tokens, the cost starts to\n59:16\nstarts to decline about a factor of 10x every year. The 10x decline every year\n59:21\nis actually telling you something different. It's saying that the race is so intense. Everybody's trying to get to\n59:28\nthe next level and somebody is getting to the next level. And so therefore, all of it is a computing problem. The faster\n59:35\nyou compute, the sooner you can get to the next level of the next frontier. All of these things are simultaneously\n59:40\nhappening at the same time. And so we decided that we have to advance\n59:47\nthe state-of-the-art of computation every single year. Not one year left\n59:52\nbehind. And now we've been shipping GB200s\n59:57\nyear and a half ago. Right now we're in fullscale manufacturing of GB300.\n1:00:03\nAnd if Vera Rubin is going to be in time for this year, it must be in production\n1:00:09\nby now. And so today I can tell you that Vera Rubin is in full production.\n1:00:20\nYou guys want to take a look at Vera Rubin? All right. Come on. Play it, please.\n1:00:28\nVera Rubin arrives just in time for the next frontier of AI.\n1:00:34\nThis is the story of how we built it. The architecture, a system of six chips\n1:00:40\nengineered to work as one, born from extreme code design. It begins with Vera, a custom-designed CPU, double the\n1:00:47\nperformance of the previous generation. And the Reuben GPU, Vera and Reuben are\n1:00:53\nco-designed from the start to birectionally and coherently share data faster and with lower latency.\n1:01:00\nThen 17,000 components come together on a Ver Rubin compute board.\n1:01:08\nHigh-speed robots place components with micron precision before the Vera CPU and\n1:01:14\ntwo Reuben GPUs complete the assembly capable of delivering 100 pedlops of AI,\n1:01:22\nfive times that of its predecessor. AI needs data fast.\n1:01:29\nConnect X9 delivers 1.6 6 terabs per second of scale out bandwidth to each\n1:01:35\nGPU. Bluefield 4 DPU offloads storage and\n1:01:41\nsecurity so compute stays fully focused on AI. The Vera Rubin compute tray completely\n1:01:48\nredesigned with no cables, hoses, or fans. Featuring a Bluefield 4 DPU, eight\n1:01:56\nConnect X9 Nix, two Vera CPUs, and four Reuben GPUs, the compute building block\n1:02:03\nof the Vera Rubin AI supercomputer. Next, the sixth generation MVLink\n1:02:10\nswitch, moving more data than the global internet, connecting 18 compute nodes,\n1:02:16\nscaling up to 72 Reuben GPUs, operating as one.\n1:02:23\nThen Spectrum X Ethernet Photonix,\n1:02:28\nthe world's first Ethernet switch with 512 lanes and 200 Gbit capable\n1:02:34\nco-packaged optics scale out thousands of racks into an AI factory.\n1:02:41\n15,000 engineer years since design began, the first Vera Rubin MVL 72 rack\n1:02:48\ncomes online. Six breakthrough chips, 18 compute trays, nine MVLink switch trays,\n1:02:55\n220 trillion transistors weighing nearly two tons.\n1:03:02\nOne giant leap to the next frontier of AI. Reuben is here.\n1:03:14\nWhat do you guys think? This is a Reuben pod. 1152 GPUs\n1:03:25\nin 16 racks. Each one of the racks as you know has uh 72\n1:03:34\nVera Rubin or 72 Reubins. Each one of the Reubins is two actual GPU dies\n1:03:41\nconnected together. And I'm going to show I'm going to show it to you, but there are several things that Well, I'll\n1:03:48\ntell you later. I can't tell you everything right away.\n1:03:55\nWell, we designed six different chips. First of all, we have a rule inside our company, and it's a good rule. No new\n1:04:01\ngeneration should have more than one or two chips change. But the problem is\n1:04:07\nthis. As you could see, we were describing the total number of transistors in each one of the chips\n1:04:12\nthat were being described. And we know that Moore's law has largely slowed. And so, the number of transistors we can get\n1:04:19\nyear after year after year can't possibly keep up with the 10 times\n1:04:25\nlarger models. It can't possibly keep up with five times per year more tokens\n1:04:31\ngenerated. It can't possibly keep up with the fact that cost decline of the\n1:04:36\ntokens are going to be so aggressive. It is impossible to keep up with those kind of rates if the indust for the industry\n1:04:44\nto continue to advance unless we deployed aggressive extreme code design\n1:04:50\nbasically innovating across all of the chips across the entire stack all at the\n1:04:55\nsame time. which is the reason why we decided that this generation we had no choice but to design every chip over\n1:05:02\nagain. Now every single chip that we were describing just now can be a press conference in all in itself and there's\n1:05:09\nan entire company who's probably dedicated to doing that back in the old days. Each one of them are completely\n1:05:14\nrevolutionary and the best of its kind. The Vera CPU I'm so proud of it in a\n1:05:21\npower constrained world. Gray CPU is two times the performance in a power\n1:05:28\nconstrained world. It's twice the performance per watt of the world's most advanced CPUs. Its data rate is insane.\n1:05:35\nIt was designed to process supercomputers and Vera was an\n1:05:41\nincredible GPU. Grace was an incredible GPU. Now Vera increases the\n1:05:47\nsinglethreaded performance, increases the capacity of the memory, increases everything just dramatically. It's a\n1:05:53\ngiant chip. This is the Vera CPU. This is one CPU.\n1:05:59\nAnd this is connected to the Reuben GPU. Look at that thing.\n1:06:08\nIt's a giant chip. Now, the thing that's really special, and I I'll go through these. It's going to take three hands, I\n1:06:15\nthink, four hands to do this. Okay. So, this is the Verus Ver CPU. It's got 88\n1:06:20\nCPU cores. And the CPU cores are designed to be multi-threaded. But the multi-threaded nature of of uh Vera was\n1:06:28\ndesigned so that each one of the 176 threads could get its full full\n1:06:33\nperformance. So it's essentially as of there's 176 cores but only 88 physical cores. So these cores were designed in\n1:06:40\nin using a technology called spatial multi-threading. But the IO performance is incredible. This is the Reuben GPU.\n1:06:47\nIt's 5x Blackwell in floating performance. But the important thing is go to the bottom line. The bottom line,\n1:06:53\nit's only 1.6 times the number of transistors of Blackwell. That kind of tells you something about the the levels\n1:07:00\nof semiconductor physics today. If we don't do code design, if we do don't do\n1:07:05\nextreme code design at the level of basically every single chip across the entire system, how is it\n1:07:13\npossible we deliver performance levels that is, you know, at best one point 1\n1:07:18\n1.6 times each year? Because that's the total number of transistors you have. And even if you were to have a little\n1:07:24\nbit more um performance per transistor, say 25%, you're this impossible to get a\n1:07:29\n100% yield out of the number of transistors you get. And so 1.6x kind of puts a ceiling on how far performance\n1:07:36\ncan go each year unless you do something extreme. And we call it extreme code design. Well, one of the things that one\n1:07:42\nof the things that we did and it was a great invention. It's called MVF FP4 tensor core. The transformer engine\n1:07:49\ninside our chip is not just a 4bit floatingoint number somehow that we put into the data path. It is an entire\n1:07:56\nprocessor, a processing unit that understands how to dynamically\n1:08:02\nadaptively adjust its precision and structure to deal with different levels\n1:08:08\nof the transformer so that you can achieve higher throughput wherever it's possible to lose precision and to go\n1:08:15\nback to the highest possible precision wherever you need to. that ability to dynamically do that. You can't do this\n1:08:22\nin software because obviously it's just running too fast. And so you have to be able to do it adaptively inside the\n1:08:29\nprocessor. That's what an MV FP4 is. When somebody says FP4 or FP8, it almost\n1:08:35\nmeans nothing to us. And the reason for that is because it's the tensor core structure in all of the algorithms that\n1:08:41\nmakes makes it work. MVF FP4, we've published papers on this already. The precision that the the level of\n1:08:47\nthroughput and precision it's able to retain is in and completely incredible. This is groundbreaking work. I would not\n1:08:53\nbe surprised that the industry would like us to make this format and this structure an industry standard in the\n1:08:59\nfuture. This is completely revolutionary. This is how we were able to deliver such a gigantic step up in\n1:09:05\nperformance even though we only have 1.6 times the number of transistors. Okay.\n1:09:11\nSo this is and now once you have a great processing node and this is the processor node and inside so this is\n1:09:19\nthis is for example here let me do this\n1:09:26\nthis is this is wow it's super heavy you have to be a CEO in really good shape to\n1:09:33\ndo this job okay all right so this thing is I'm\n1:09:41\ngonna guess this is probably I don't know couple of hundred pounds.\n1:09:48\n[Laughter] I thought that was funny.\n1:09:54\nCome on. It could have been. Everybody's going. I don't think so.\n1:10:00\nAll right. So So look at this. This is the last one. We revolutionized the\n1:10:05\nentire MGX chassis. This node 43 cables,\n1:10:13\nzero cables, six tubes,\n1:10:18\nz just two of them here. It takes 2 hours to assemble this. If you're lucky,\n1:10:26\nit takes two hours. And of course, you're probably going to assemble it wrong. You're going to have to retest it, test it, reassemble it. So, the\n1:10:33\nassembly process is incredibly complicated. And it was understandable as one of our first supercomputers\n1:10:39\nthat's deconstructed in this way. This from two hours to five minutes.\n1:10:50\n80% liquid cooled. 100% liquid cooled.\n1:10:56\nYeah. Really, really a breakthrough. Okay. So, so this is the new compute\n1:11:02\nchassis and what connects all of these to the top of rack switches. The east\n1:11:08\nwest traffic is called the spectrox Nick. This is the world's best nick.\n1:11:14\nUnquestionably Nvidia's Melanox, the acquisition Melanox that joined us a long time ago now. Um, this their\n1:11:20\nnetworking technology for high performance computing is the world's best bar none. the algorithms, the chip\n1:11:26\ndesign, all of the interconnects, all the software stacks that run on top of it, their RDMA, absolutely absolutely\n1:11:33\nbar none, the world's best. And now it has the ability to do programmable RDMA and data path accelerator so that our\n1:11:39\npartners like AI labs could create their own algorithms for how they want to move data around the system. But this is\n1:11:46\ncompletely world worldclass connect X9 and the Vera CPU were co-designed and\n1:11:52\nwe never revealed it. not never never released it until CX9 came along because\n1:11:58\nwe we co-designed it for a new type of processor. You know, Connect X9 or CX8 and Spectrum\n1:12:06\nX revolutionized how Ethernet was done for artificial intelligence. Ethernet\n1:12:12\ntraffic for AI is much much more intense, requires much lower latency.\n1:12:17\nThe the instantaneous surge of traffic is unlike anything Ethernet sees. And so\n1:12:22\nwe created Spectrum X which is AI Ethernet. Two years ago we announced Spectrum X.\n1:12:30\nNVIDIA today is the largest networking company the world has ever seen. So it's\n1:12:35\nbeen so successful and used in so many different installations. It is just sweeping uh the AI landscape. The\n1:12:42\nperformance is incredible especially when you have a 200 um megawatt data center or if you have a\n1:12:50\ngigawatt data center. These are billions of dollars. Let's say a gigawatt data center is $50 billion dollars. If the\n1:12:57\nnetworking performance allows you to deliver an extra 10%\n1:13:03\nin the case of Spectrum X delivering 25% higher throughput is not uncommon. If we\n1:13:09\nwere to just deliver 10% that's worth $5 billion. The networking is completely\n1:13:14\nfree which is the reason why well everybody uses Spectrum X. It's just an\n1:13:20\nincredible thing. And now we're going to invent a new type a new type of uh uh\n1:13:25\ndata processing. And so Spectrox is for east west traffic. We now have a new\n1:13:31\nprocessor called Bluefield 4. Bluefield 4 allows us to take a large large very large data center isolate different\n1:13:38\nparts of it so that different users could use different parts of it. Make sure that everything could be virtualized if they decide to be\n1:13:44\nvirtualized. So you offload a lot of the um virtualization software, the security\n1:13:49\nsoftware, the networking software for your north south traffic. And so Bluefield 4 comes standard with every\n1:13:56\nsingle one of these compute nodes. Bluefield 4 has a second application I'm going to talk about in just a second.\n1:14:02\nThis is a revolutionary processor and I'm so excited about it. This is the MVLink 6 switch\n1:14:09\nand it's right here. This is the this\n1:14:14\nswitch. This switch chip there are four of them inside the MVLink switch here.\n1:14:20\nEach one of these switchips has the fastest certis in history. The world is\n1:14:25\nbarely getting to 200 Gbits. This is 400 Gbits per second switch. The reason why\n1:14:32\nthis is so important is so that we could have every single GPU talk to every other GPU at exactly the same time. This\n1:14:39\nswitch, this switch on the back plane of one of these racks enables us to move\n1:14:47\nthe equivalent of twice the amount of the global internet data,\n1:14:54\ntwice as all of the world's internet data at twice the speed. You take the\n1:14:59\ncross-sectional bandwidth of the entire planet's internet, it's about 100 terabytes per second. This is 240\n1:15:06\nterabytes per second. So it kind of puts it in perspective. This is so that every single GPU can work with every single\n1:15:12\nother GPU at exactly the same time. Okay. Then on top of that\n1:15:19\non top of that okay so this is one rack. This is one rack. Each one of the racks as you could see the number of\n1:15:25\ntransistors in this one rack is 1.7 times.\n1:15:34\nYeah. Could you do this for me? So, this is it's usually about two tons, but\n1:15:40\ntoday it's two and a half tons because um when they shipped it, they forgot to drain the water out of it.\n1:15:48\nSo, we we shipped a lot of water from California.\n1:15:55\nCan you hear it squealing? You know, when you're rotating two and a half tons,\n1:16:01\nyou're going to squeal a little. Oh, you could do it. Wow.\n1:16:09\nOkay, we just we won't make you do that twice. All right. So, so um so behind\n1:16:15\nbehind this are the MVLink spines. Basically, two miles of copper cables.\n1:16:22\nCopper is the best conductor we know. And these are all shielded copper cables, structured copper cables, the\n1:16:28\nmost the world's ever used in computing systems ever. and and um uh our certis\n1:16:35\ndrive the copper cables from the top of the rack all the way to the bottom of the rack at 400 gigabits per second.\n1:16:41\nIt's incredible. And so uh this has 2 miles of total copper cables, 5,000 copper cables, and this makes the MVLink\n1:16:50\nuh spine possible. This is the revolution that that really started the NGX system. Now we we decided that we\n1:16:59\nwould create an industry standard system so that the entire ecosystem all of our supply chain could standardize on these\n1:17:06\ncomponents. There some 80,000 different components that make up this\n1:17:13\nthese MGX systems and it's a total waste if we're to change it every single year. every single major computer company from\n1:17:20\nFoxcon to Quanta to Wistron, you know, the list goes on and on and on to HP and\n1:17:25\nDell and Lenovo, everybody knows how to build these systems. And so the fact that we could squeeze Ruben, Vera Rubin\n1:17:34\ninto this even though the performance is so much so much higher and very\n1:17:39\nimportantly the power is twice as high. The power of Vera Rubin is twice as high\n1:17:44\nas Grace Blackwell. And yet, and this is the miracle,\n1:17:50\nthe air that goes into it, the the air flow is about the same. And very\n1:17:55\nimportantly, the water that goes into it is the same temperature, 45\u00b0 C. With 45\u00b0\n1:18:01\nC, no water chillers are necessary for data centers. We're basically cooling\n1:18:08\nthis supercomput with hot water. is so incredibly efficient. And so\n1:18:15\nthis is um this is the new the new rack. 1.7 times more transistors but five\n1:18:21\ntimes more peak inference performance. Three and a half times more peak um uh\n1:18:28\nuh training performance. Okay. They're connected on top using Spectrum\n1:18:34\nX. Oh, thank you. [Applause]\n1:18:42\nThis is this is the world's first manufacturing chip using uh TSMC's\n1:18:49\nnew process that we co-inovated called coupe. It's a silicon photonix integrated silicon photonix process\n1:18:55\ntechnology. And this allows us to take silicon photonix directly right to the\n1:19:00\nchip. And this is 512 ports at 200 gigabits per second. And this is the new\n1:19:08\nEthernet AI switch, the Spectrum X Ethernet switch. And look at this giant chip. But what's really amazing, it's\n1:19:15\ngot silicon photonics directly connected to it. And lasers come in.\n1:19:23\nLasers come in through here. Lasers come in through here. The optics are here and\n1:19:28\nthey connect out to the rest of the data data center. This I'll show you in a second, but this is on top of the rack.\n1:19:35\nAnd this is the new Spectrumax um silicon photonix switch. Okay.\n1:19:44\nAnd we have something new I want to tell you about. So just as I mentioned a couple years ago,\n1:19:51\nwe introduced Spectrum X so that we could reinvent the way that networking is done. Um Ethernet is really easy to\n1:19:58\nmanage and everybody has an Ethernet stack and every data center in the world knows how to deal with Ethernet. Um and\n1:20:04\nthe only thing that we were we were using at the time was called Infiniband which is used for supercomputers.\n1:20:10\nInfiniband is very low latency. Um but of course the software stack the entire\n1:20:15\nmanageability of Infiniband is very alien to the people who use Ethernet. So we decided to enter the Ethernet switch\n1:20:21\nmarket for the very first time. Spectrum X that just took off and it made us the\n1:20:27\nlargest networking company in the world. As I mentioned, this next generation Spectrum X is going to carry on that\n1:20:33\ntradition. But just as I said earlier, AI has reinvented the whole computing\n1:20:38\nstack, every layer of the computing stack. It stands to reason that when AI\n1:20:43\nstarts to get deployed in the world's enterprises, it's going to also reinvent the way storage is done. Well, AI\n1:20:49\ndoesn't use SQL. AI use semantics information. And when AI is being used,\n1:20:55\nit creates this temporary knowledge, temporary temporary memory calls KV cache, K key value combinations, but\n1:21:04\nit's a KV cache. Basically, the cache of the AI, the working memory of the AI. And the working memory of the AI is\n1:21:10\nstored in the HBM memory. every single token. For every single token,\n1:21:17\nthe H the GPU reads in the model, the entire model, it reads in the entire\n1:21:24\nworking memory and it produces one token and it stores that one token back into\n1:21:30\nthe KV cache. And then the next to the next time it does that, it reads in the entire memory, reads it and it streams\n1:21:38\nit through our GPU and then generates another token. Well, it does this repeatedly, token after token after\n1:21:44\ntoken. And obviously, if you have a long conversation with that AI over time, that memory, that context memory is\n1:21:50\ngoing to grow tremendously. Not to mention, the models are growing. The number of turns that we're using, the AIS are are increasing. We would like to\n1:21:57\nhave this AI stay with us our entire life. And remember, every single conversation we've ever had with it,\n1:22:03\nright? Every single lick of research that I've asked it for. Of course, with the number of people that will be\n1:22:08\nsharing the supercomputer is going to continue to grow. And so this context memory which started out fitting inside\n1:22:14\nan HBM is no longer large enough. Last year we created Grace Blackwell's\n1:22:22\nvery fast memory we called fast context memory in that's the reason why we\n1:22:27\nconnected Grace directly to Hopper. That's why we connected Grace directly to Blackwell so that we can expand the\n1:22:34\ncontext memory. But even that is not enough. And so the next solution of course is to go off onto the network the\n1:22:41\nnorth south network off to the storage of the company. But if you have a whole\n1:22:47\nlot of AI running at the same time that network is no longer going to be fast enough. So the answer is very clearly to\n1:22:55\ndo it different. And so we intro we created Bluefield 4 so that we could essentially have a very fast KV cache\n1:23:03\ncontext memory store right in the rack. And so I'll show you in just one second.\n1:23:10\nBut there's a whole new category of storage systems and the industry is so excited because this is a pain point for\n1:23:17\njust about everybody who does a lot of token generation today. the AI labs, the cloud service providers, they're really\n1:23:24\nsuffering from the amount of network traffic that's causing being caused by KV cache moving around. And so the idea\n1:23:31\nthat we would create a new platform, a new processor to run the entire Dynamo\n1:23:38\nKV cache context memory management system and to put it very close to the\n1:23:43\nrest of the rack is completely revolutionary. So this is it. This is it\n1:23:48\nsits right here. So this this is all the compute nodes.\n1:23:54\nEach one of these is MVLink 72. So this is Vera Rubin MVLink 72\n1:24:01\n144 U Reuben GPUs. This is the context memory that's stored here. Behind each\n1:24:08\none of these are four blue fields. Behind each blue field is 150 gigab 150\n1:24:14\nterabytes 150 terabytes of memory context memory. And for each GPU once you allocate it\n1:24:22\nacross each GPU will get an additional 16 terabytes. Now inside this node each\n1:24:30\nGPU essentially has one terabyte. And now with this backing store here\n1:24:37\ndirectly on the same east west traffic at exactly the same data rate 200 gigabits per second across literally the\n1:24:45\nentire fabric of this compute node. you're going to get an additional 16 terabytes of memory. Okay. And this is\n1:24:53\nthe management plane. These are these are the spectrum X\n1:25:01\nswitches that connects all of them together. And over here, these switches at the end\n1:25:10\nconnects them to the rest of the data center. Okay? And so this is the Vera\n1:25:15\nRubin. Now there's several things that's really incredible about it. So the first thing that I mentioned is that the this\n1:25:23\nentire system is twice the energy efficiency essentially the the twice\n1:25:30\nthat the the temperature performance in the sense that that even though the power is twice as high the amount of\n1:25:37\nenergy used is twice as high the amount of computation is many times higher than that but the liquid that goes into it\n1:25:44\nstill 45 degrees C that enables us to save about 6% % of the world's data\n1:25:49\ncenter power. So that's a very big deal. The second very big deal\n1:25:55\nis that this entire system is now confidential computing safe. Meaning everything is encoded in transit at rest\n1:26:02\nand during compute and every single bus is now encrypted. every PCI Express,\n1:26:09\nevery MV link, every H you know for MV link between CPU me and GPU between GPU\n1:26:15\nto GPU, everything is now encrypted and so it's confidential computing safe.\n1:26:22\nThis allows companies to feel safe that their models are being deployed by\n1:26:27\nsomebody else, but it will never be seen by anybody else. Okay? And so this particular system is not only incredibly\n1:26:35\nenergy efficient and there's one other thing that's incredible because of the nature of the workload of\n1:26:42\nAI it spikes instantaneously with this computation layer called all\n1:26:47\nreduce the amount of current the amount of energy that is used sp simultaneously\n1:26:54\nis really off the charts oftentimes it'll spike up 25%. We now have power\n1:26:59\nsmoothing across the entire system so that you don't have to overprovision by\n1:27:05\n25 times or if you overprovision by 25 times you don't have to leave 25 times\n1:27:11\n25% not 25 times 25% of the energy um\n1:27:17\nsquandered or unused and so now you could fill up the entire power budget\n1:27:22\nand you don't have to over you don't have to proceed you don't have to provision beyond that and then the Last\n1:27:28\nthing of course is performance. So let's take a look at the performance of this. These are only charts that people who\n1:27:34\nbuild AI super supercomputers would love. It took exact it took every single\n1:27:39\none of these chips complete redesign of every single one of the systems and rewriting the entire stack for us to\n1:27:45\nmake this possible. Basically this is training the AI model. This\n1:27:50\nfirst column, the faster you train AI models, the faster you can get the next\n1:27:56\nfrontier out to the world. This is your time to market. This is technology leadership. This is your pricing power.\n1:28:02\nAnd so in the case of the green, this is essentially\n1:28:08\na 10 trillion parameter model. We scaled it up from deepse. That's why we call it\n1:28:14\ndeepseat++. A training a 10 trillion parameter model on a 100 trillion\n1:28:20\ntokens. Okay. And that's this is our simulation projection of what it would\n1:28:25\ntake for us to build the next frontier model. The next frontier model uh Elon's already mentioned that the next version\n1:28:31\nof Grock Grock 5 I think is 7 trillion parameters. So this is 10 and in the green is black. Well, and here in the\n1:28:39\ncase of um uh Reuben, notice the throughput is so much higher and\n1:28:46\ntherefore it only takes 1/4th as many of these systems in order to train the\n1:28:52\nmodel in the time that we gave it here which is one month. Okay. And so time\n1:28:58\ntime is the same for everybody. Now how much how fast you can train that model and how large of a model you can train\n1:29:04\nis how you're going to get to the frontier first. The second part is your factory throughput.\n1:29:10\nBlackwell is green again. And factory throughput is important because your factory is in the case of a gigawatt is\n1:29:17\n$50 billion. A $50 billion data center\n1:29:22\ncan only consume one gawatt of power. And so if your performance,\n1:29:29\nyour throughput per watt is very good versus quite poor, that directly\n1:29:35\ntranslates to your revenues. Your revenues of your data center is directly\n1:29:41\nrelated to the second second column. And in the case of Blackwell, it was about\n1:29:46\n10 times over Hopper. In the case of Reuben, it's going to be about 10 times higher again. Okay. And in the case of\n1:29:53\nnow the um the cost of the tokens, how\n1:29:58\ncost effectively it is to generate the token. This is Reuben about onetenth\n1:30:04\njust as in the case of Yep.\n1:30:12\nSo that's how this is how we're going to get everybody to the next frontier to um push AI to the next level and of\n1:30:21\ncourse to build these data centers energy efficiently and costefficiently.\n1:30:26\nSo this is it. This is Nvidia today. You know, we mentioned that we build chips,\n1:30:33\nbut as you know, Nvidia builds entire systems now. And AI is a full stack. We\n1:30:40\nwe're reinventing AI across everything from chips to infrastructure to models\n1:30:46\nto applications. And our job is to create the entire stack so that all of you could create incredible applications\n1:30:53\nuh for the rest of the world. Thank you all for coming. Have a great CES. Now,\n1:31:00\nbefore before I before I let you guys go, uh there were a whole bunch of slides we have to cut we had to leave on\n1:31:06\nthe cutting floor and so we have some out takes here. I think it'll be fun for you. Have a great see us, guys.\n1:31:17\nAnd cut Nvidia live at CES. Take four marker.\n1:31:25\nBoom. like action.\n1:31:30\nSorry guys. Platform shift, huh? [Music]\n1:31:37\nThat should do it. And let's roll camera. [Music]\n1:31:43\nA shade of green. A bright happy green. World's most powerful AI supercomput you\n1:31:51\ncan plug into the wall next to my toaster.\n1:31:57\nHey guys, I'm I'm stuck again. I'm so sorry. This slide is never going to work. Let's just cut it. Hello. Can you hear me?\n1:32:07\nSo, like I was saying, the router. Because not every problem needs the biggest, smartest model. Just the right\n1:32:14\none. No. No, don't lose any of them. This new six chip Reuben platform makes\n1:32:22\none amazing AI supercomputer. There you go, little guy. Oh no, no, not\n1:32:30\nthe scaling laws. There is a squirrel on the car. Be ready to make the squirrel go away. Ask the\n1:32:36\nsquirrel gently to move away. Did you know the best models today are all mixture of experts?\n1:32:45\nHey, [Music] [Applause] [Music]\n1:32:54\n[Music] where'd everybody go?\n1:33:02\n[Music]\n1:33:16\n[Music]\n",
      "added_at": 1768361232.186916,
      "tokens": 18965
    },
    {
      "name": "samsung_booth_demo_no_transcript.txt",
      "category": "documentation",
      "size_kb": 11.7,
      "content": "SAMSUNG BOOTH DEMO - CES 2026 DAY 1 (NO TRANSCRIPT)\nVideo Analysis Report | January 7, 2026 | 15 minutes\nSource: YouTube (@TechVloggerCES) - Uploaded 3 hours ago\nProcessing Method: Multi-Model Vision + Audio Analysis\n\n[PROCESSING METADATA]\nVideo URL: youtube.com/watch?v=dQw4w9WgXcQ\nDuration: 14:37\nUpload Time: 2026-01-07 11:23 PST (3 hours ago)\nTranscript Available: NO (too recent for auto-captions)\nProcessing Required: URGENT - Competitive intelligence, same-day response needed\n\n[MULTI-MODEL PROCESSING PIPELINE]\n\nSTEP 1: FRAME EXTRACTION (Vision Model: LLaVA-13B)\n- Extracted 87 keyframes at 10-second intervals\n- Model loaded: LLaVA-13B (13GB VRAM)\n- Processing time: 4.2 minutes\n- Output: Frame descriptions + slide text extraction\n\nSTEP 2: AUDIO TRANSCRIPTION (Whisper Large-v3)\n- Model swap: Offloaded LLaVA-13B, loaded Whisper Large-v3 (3GB VRAM)\n- Transcribed 14:37 of audio\n- Processing time: 2.1 minutes\n- Output: Full transcript with timestamps\n\nSTEP 3: CONTENT ANALYSIS (Llama 3.1 8B)\n- Model swap: Offloaded Whisper, loaded Llama 3.1 8B (5GB VRAM)\n- Cross-referenced visual + audio content\n- Processing time: 3.5 minutes\n- Output: Strategic intelligence summary\n\nTotal Processing Time: 9.8 minutes (with model swapping)\nTotal Memory Peak: 13GB (LLaVA frame extraction phase)\n\n---\n\n[EXTRACTED CONTENT - FRAME ANALYSIS]\n\nFRAME 00:15 - Booth Entrance\nVisual: Samsung PM9E1 SSD banner, \"AI-Native Storage for On-Device AI\"\nText Detected: \"5nm Controller | PCIe Gen 5.0 | Optimized for AI Workloads\"\n\nFRAME 00:45 - Product Display\nVisual: PM9E1 SSD physical unit on pedestal\nText Detected: \"14.5 GB/s Read | 13.0 GB/s Write | 3.0M IOPS\"\n\nFRAME 01:30 - Presentation Slide 1\nVisual: Samsung presenter at screen\nSlide Title: \"PM9E1: The AI SSD Revolution\"\nSlide Content:\n- \"In-house 5nm controller design\"\n- \"AI workload optimization\"\n- \"Mixed read/write pattern handling\"\n- \"Thermal management for dense deployments\"\n\nFRAME 02:15 - Presentation Slide 2\nSlide Title: \"Competitive Positioning\"\nSlide Content:\n- \"Vertical integration advantage\"\n- \"Own NAND + Controller = Faster time-to-market\"\n- \"Supply chain control in shortage environment\"\n[STRATEGIC NOTE: Direct competitive claim vs. Phison's multi-source approach]\n\nFRAME 03:00 - Presentation Slide 3\nSlide Title: \"AI PC Market Opportunity\"\nChart Detected: Bar graph showing \"AI PC SSD Attach Rate 2024-2027\"\n- 2024: 12%\n- 2025: 28%\n- 2026: 51% (forecast)\n- 2027: 73% (forecast)\n\nFRAME 04:30 - Demo Setup\nVisual: Laptop running AI inference benchmark\nText Overlay: \"Real-time LLM Inference Demo\"\nScreen shows: \"Llama 3.1 70B | 32K Context | PM9E1 SSD\"\n\nFRAME 05:15 - Performance Metrics\nVisual: Benchmark results on screen\nMetrics Displayed:\n- \"Tokens/sec: 8.2\"\n- \"Latency: 122ms\"\n- \"Memory Offload: 18GB to SSD\"\n[STRATEGIC NOTE: Demonstrates SSD-based memory offload - validates aiDAPTIV+ concept]\n\nFRAME 06:45 - Presentation Slide 4\nSlide Title: \"Memory Hierarchy for AI\"\nDiagram Detected: Pyramid showing:\n- Top: \"GPU HBM (80GB)\"\n- Middle: \"System DRAM (128GB)\"\n- Bottom: \"PM9E1 SSD Cache (2TB)\"\nText: \"Intelligent tiering for cost-effective AI\"\n\nFRAME 08:00 - Q&A Session\nVisual: Audience member asking question\n[Audio transcription needed for question content]\n\nFRAME 09:30 - Presentation Slide 5\nSlide Title: \"Roadmap 2026-2027\"\nContent:\n- \"Q2 2026: PM9E1 OEM partnerships\"\n- \"Q3 2026: Consumer variants\"\n- \"Q4 2026: Next-gen controller (3nm)\"\n- \"2027: CXL memory fabric integration\"\n\nFRAME 11:00 - Competitive Comparison Slide\nSlide Title: \"PM9E1 vs. Competition\"\nTable Detected:\n| Feature | PM9E1 | Competitor A | Competitor B |\n|---------|-------|--------------|--------------|\n| Read Speed | 14.5 GB/s | 14.0 GB/s | 13.5 GB/s |\n| AI Optimization | Yes | Limited | No |\n| Vertical Integration | Yes | No | No |\n[STRATEGIC NOTE: \"Competitor A\" likely Phison-based solutions]\n\nFRAME 12:30 - Pricing Discussion\nVisual: Slide with pricing tiers\nText Detected: \"Competitive pricing vs. market alternatives\"\n[Specific prices not shown - likely NDA material]\n\nFRAME 13:45 - Closing Slide\nText: \"Samsung Memory Solutions Lab - Innovating for AI Era\"\nContact: \"memory-solutions@samsung.com\"\n\n---\n\n[AUDIO TRANSCRIPT - WHISPER LARGE-V3]\n\n[00:00:15]\nPRESENTER: \"Welcome to Samsung's AI SSD showcase. I'm Dr. Kim, lead engineer for the PM9E1 program. Today I'll show you why we believe the PM9E1 is the future of AI storage.\"\n\n[00:01:30]\nPRESENTER: \"Let's start with the fundamentals. AI workloads are fundamentally different from traditional storage patterns. You have massive sequential reads for model loading, random writes for checkpointing, and mixed patterns for inference caching. Traditional SSDs weren't designed for this.\"\n\n[00:02:45]\nPRESENTER: \"Our 5nm controller is purpose-built for AI. We've optimized the firmware for KV cache patterns, implemented intelligent write buffering, and added thermal management specifically for dense AI server deployments.\"\n\n[00:03:30]\nPRESENTER: \"Now, you might ask - why does Samsung have an advantage here? It's simple: vertical integration. We make the NAND, we design the controller, we control the entire stack. This means faster iteration, better optimization, and crucially - supply security.\"\n\n[00:04:15]\nPRESENTER: \"In today's market, where NAND is constrained and controller lead times are extending, having control over your supply chain is a competitive weapon.\"\n\n[STRATEGIC NOTE: Direct shot at fabless controller companies like Phison]\n\n[00:05:00]\nPRESENTER: \"Let me show you a live demo. This is a standard laptop running a 70-billion parameter language model. Notice the memory configuration - 32GB of system RAM. Normally, this wouldn't be enough. But watch what happens when we enable SSD-based memory offload...\"\n\n[00:05:45]\nPRESENTER: \"The system is now offloading 18 gigabytes of KV cache to the PM9E1. And we're still getting 8 tokens per second - that's usable performance for real-world applications.\"\n\n[00:06:30]\nAUDIENCE MEMBER: \"How does this compare to just adding more DRAM?\"\n\n[00:06:35]\nPRESENTER: \"Great question. Adding DRAM would be faster, yes. But it's also 10x more expensive per gigabyte, and in today's market, you often can't even get the capacity you need. Our approach is about pragmatic solutions for real-world constraints.\"\n\n[00:07:15]\nPRESENTER: \"Think about it - a 2TB PM9E1 costs maybe $200. The equivalent in DRAM? That's $2000-plus, assuming you can even find it. For AI workloads that need massive capacity, SSD-based offload is the only economically viable option.\"\n\n[00:08:00]\nAUDIENCE MEMBER: \"What about competitors doing similar things?\"\n\n[00:08:05]\nPRESENTER: \"There are definitely other companies exploring this space. Some controller manufacturers are adding AI-specific features. But here's the difference - they're dependent on NAND suppliers, they don't control the full stack, and they're competing for the same constrained supply we are.\"\n\n[00:08:30]\nPRESENTER: \"Samsung's advantage is that we make the NAND. When supply is tight, we can prioritize our own controller division. That's a structural advantage that's hard to replicate.\"\n\n[STRATEGIC NOTE: Acknowledges Phison-type competitors but claims supply chain moat]\n\n[00:09:00]\nPRESENTER: \"Let me show you our roadmap. Q2 2026, we're launching OEM partnerships - expect to see PM9E1 in major AI PC brands. Q3, consumer variants. Q4, we're moving to 3nm controller design for even better power efficiency.\"\n\n[00:09:45]\nPRESENTER: \"And looking further out - 2027 - we're working on CXL memory fabric integration. This will let SSDs participate directly in the memory coherency domain, making them true memory-tier devices, not just storage.\"\n\n[00:10:30]\nAUDIENCE MEMBER: \"Is the market big enough for this? How many AI PCs are we really talking about?\"\n\n[00:10:35]\nPRESENTER: \"Our forecast shows AI PC attach rate hitting 51% in 2026, 73% in 2027. That's tens of millions of units. And each one needs specialized storage. This isn't a niche - this is the mainstream PC market transforming.\"\n\n[00:11:15]\nPRESENTER: \"Let me show you a competitive comparison. [Points to slide] We're faster than Competitor A, we have AI optimizations that Competitor B lacks, and we have the vertical integration advantage that nobody else can match.\"\n\n[00:11:45]\nAUDIENCE MEMBER: \"Who is Competitor A?\"\n\n[00:11:48]\nPRESENTER: [Laughs] \"I can't name names, but if you look at the market, there are a few obvious players. Some are fabless controller companies partnering with NAND makers. Some are other vertically integrated players. We respect the competition, but we think our approach is superior.\"\n\n[00:12:15]\nPRESENTER: \"Pricing - I can't give you specific numbers here, but we're committed to competitive pricing. We're not trying to extract monopoly rents. We want to enable the AI PC revolution, and that means accessible pricing.\"\n\n[00:13:00]\nPRESENTER: \"Any other questions? [Pause] Alright, thank you all for coming to our booth. If you want more details, grab a card on your way out, or email our Memory Solutions Lab. We're excited to be part of the AI storage revolution.\"\n\n[00:13:30]\n[Applause]\n\n[00:14:00]\nATTENDEE (off-camera): \"Hey, can you talk more about the thermal management?\"\n\n[00:14:05]\nPRESENTER: \"Sure! So in dense AI deployments, you might have 10-20 SSDs in a single chassis, all running hot workloads. We've implemented dynamic voltage and frequency scaling - DVFS - that throttles performance when temps spike, preventing thermal runaway. It's a big deal for server deployments.\"\n\n[00:14:30]\n[Video ends]\n\n---\n\n[STRATEGIC INTELLIGENCE SUMMARY - LLAMA 3.1 8B ANALYSIS]\n\nCOMPETITIVE THREAT LEVEL: HIGH\n\nSamsung PM9E1 represents a direct competitive threat to Phison's E26-based AI SSD strategy. Key concerns:\n\n1. VERTICAL INTEGRATION ADVANTAGE\n   - Samsung controls NAND supply, giving them structural advantage in shortage environment\n   - Can prioritize internal controller division over external customers\n   - Faster time-to-market for new NAND generations\n\n2. AI-SPECIFIC OPTIMIZATIONS\n   - Purpose-built 5nm controller for AI workloads\n   - Firmware optimized for KV cache patterns\n   - Thermal management for dense deployments\n   - Directly competitive with Phison's aiDAPTIV+ value proposition\n\n3. MARKET VALIDATION\n   - Samsung's demo validates SSD-based memory offload concept\n   - Proves market demand for AI-optimized storage\n   - Confirms Phison's strategic direction is correct\n\n4. COMPETITIVE POSITIONING\n   - Samsung explicitly positioning against \"fabless controller companies\"\n   - Claims supply chain advantage during NAND shortage\n   - Aggressive OEM partnership timeline (Q2 2026)\n\nOPPORTUNITIES FOR PHISON:\n\n1. MULTI-SOURCE FLEXIBILITY\n   - Counter Samsung's vertical integration with multi-NAND-maker partnerships\n   - Position as \"vendor-neutral\" vs. Samsung's captive approach\n   - Highlight risk of single-source dependency\n\n2. ECOSYSTEM BREADTH\n   - Phison partners with Kioxia, Micron, others - broader ecosystem\n   - OEMs may prefer diversified supply vs. Samsung-only\n\n3. SOFTWARE DIFFERENTIATION\n   - aiDAPTIV+ software stack as differentiator\n   - Samsung showed hardware, less emphasis on software intelligence\n\nRECOMMENDED ACTIONS:\n\n1. IMMEDIATE: Competitive response brief on PM9E1 vs. E26 + aiDAPTIV+\n2. SHORT-TERM: Accelerate OEM partnerships to match Samsung's Q2 timeline\n3. MEDIUM-TERM: Emphasize multi-source supply advantage in marketing\n4. LONG-TERM: Develop CXL roadmap to match Samsung's 2027 plans\n\nCONFIDENCE LEVEL: HIGH (based on direct booth demo observation, presenter statements, and roadmap disclosure)\n\n---\n\n[PROCESSING NOTES]\n- Multi-model pipeline required 3 model swaps (LLaVA \u2192 Whisper \u2192 Llama)\n- Total memory footprint: 13GB peak (LLaVA phase)\n- Processing time: 9.8 minutes (acceptable for 3-hour-old urgent intelligence)\n- Without aiDAPTIV+ model swapping: Would require 21GB+ simultaneous VRAM (infeasible on consumer hardware)\n- This demonstrates real-world value of memory offload for multi-model AI workflows\n",
      "added_at": 1768361287.907446,
      "tokens": 3001
    },
    {
      "name": "strategic_context.txt",
      "category": "dossier",
      "size_kb": 6.9,
      "content": "# PHISON STRATEGIC INTELLIGENCE FRAMEWORK\n## Q1 2026 - CES Monitoring Context\n\nThis document defines the strategic lens through which market intelligence should be interpreted for Phison's aiDAPTIV+ product marketing.\n\n---\n\n## CORE THESIS\n\n**\"AI model growth is outpacing memory capacity growth. Local/edge AI requires memory offload architectures. Phison aiDAPTIV+ is the cost-effective solution.\"**\n\n---\n\n## STRATEGIC INFERENCE RULES\n\n### Rule 1: Memory Constraints = OPPORTUNITY\n**IF** news mentions:\n- DRAM shortage, price increases, supply constraints\n- AI models requiring more memory than available\n- Developers hitting VRAM/RAM limits\n- OEMs shipping PCs with limited memory (16GB, 32GB caps)\n\n**THEN** Interpretation:\n- **Sentiment**: POSITIVE for Phison\n- **Reasoning**: Memory scarcity drives demand for offload solutions\n- **Action**: Highlight aiDAPTIV+ as cost-effective alternative\n\n**Example Signals**:\n- \"SK Hynix 2026 capacity sold out\"\n- \"DRAM prices up 55-60% in Q1\"\n- \"Dell XPS ships with soldered 16GB RAM\"\n- \"Reddit: Can't run Llama 70B on my 4090\"\n\n---\n\n### Rule 2: Cloud AI Infrastructure = VALIDATION (Not Threat)\n**IF** news mentions:\n- NVIDIA datacenter memory solutions (e.g., \"Domino\", HBM innovations)\n- Cloud providers building massive AI infrastructure\n- Enterprise spending billions on AI servers\n\n**THEN** Interpretation:\n- **Sentiment**: NEUTRAL to POSITIVE (Market Validation)\n- **Reasoning**: Proves memory bottleneck is real, but cloud \u2260 edge\n- **Action**: Position as \"They solve it in cloud, we solve it at edge\"\n\n**Example Signals**:\n- \"NVIDIA Vera Rubin with 1.5TB system memory\"\n- \"NVIDIA Inference Context Memory Storage Platform\"\n- \"Microsoft Azure expanding AI capacity\"\n\n**Key Insight**: Cloud solutions validate the problem exists. They don't compete with local AI use cases (privacy, latency, cost).\n\n---\n\n### Rule 3: Flash Supply Dynamics = MIXED SIGNAL\n**IF** news mentions:\n- NAND flash shortages, price increases\n- Kioxia production updates\n- NAND manufacturers capacity constraints\n\n**THEN** Interpretation:\n- **Sentiment**: MIXED (Depends on context)\n- **Reasoning**: \n  - Bad: Higher SSD costs could hurt aiDAPTIV+ pricing\n  - Good: Phison-Kioxia partnership provides supply security advantage\n- **Action**: Monitor for competitive advantage opportunities\n\n**Example Signals**:\n- \"Kioxia 332-layer BiCS10 ramping in 2026\" \u2192 POSITIVE (partner strength)\n- \"NAND prices up 33-38% Q1 2026\" \u2192 NEUTRAL (affects all players)\n- \"Samsung reducing MLC production\" \u2192 OPPORTUNITY (supply gap)\n\n---\n\n### Rule 4: OEM Supply Chain Concerns = PARTNERSHIP OPPORTUNITY\n**IF** news mentions:\n- OEMs expressing supply chain worries\n- Component shortages affecting PC production\n- OEMs seeking reliable partners\n\n**THEN** Interpretation:\n- **Sentiment**: POSITIVE (Partnership Opportunity)\n- **Reasoning**: Phison can offer supply security via Kioxia relationship\n- **Action**: Proactive outreach to strengthen partnerships\n\n**Example Signals**:\n- \"Dell acknowledges memory supply pressures\"\n- \"HP seeking alternative suppliers\"\n- \"ASUS diversifying component sources\"\n\n---\n\n### Rule 5: Competitor Innovations = THREAT (Monitor Closely)\n**IF** news mentions:\n- Samsung/Silicon Motion/Marvell new AI-focused controllers\n- Competitive pricing announcements\n- New memory offload technologies\n\n**THEN** Interpretation:\n- **Sentiment**: NEGATIVE (Competitive Threat)\n- **Reasoning**: Direct competition for aiDAPTIV+ market\n- **Action**: Competitive analysis, feature comparison, response strategy\n\n**Example Signals**:\n- \"Samsung PM9E1 with AI optimizations\"\n- \"Silicon Motion MonTitan for AI workloads\"\n- \"Marvell announces AI SSD controller\"\n\n---\n\n### Rule 6: Developer/Grassroots Signals = EARLY WARNING SYSTEM\n**IF** social media/forums mention:\n- Developers struggling with memory limits\n- Workarounds for running large models locally\n- Complaints about cloud costs\n- Interest in on-premises AI\n\n**THEN** Interpretation:\n- **Sentiment**: POSITIVE (Market Demand Validation)\n- **Reasoning**: Grassroots pain points = product-market fit signals\n- **Action**: Engage community, provide solutions, gather feedback\n\n**Example Signals**:\n- \"Reddit r/LocalLLaMA: 24GB VRAM is the minimum\"\n- \"HackerNews: AWS bills killing my startup\"\n- \"Twitter: @karpathy on context memory challenges\"\n- \"GitHub Issues: OOM errors in LLM projects\"\n\n---\n\n### Rule 7: Apple/Premium Segment = MARKET VALIDATION\n**IF** news mentions:\n- Apple unified memory configurations\n- High-end workstations with massive RAM\n- Premium pricing for memory upgrades\n\n**THEN** Interpretation:\n- **Sentiment**: POSITIVE (Validates problem, highlights cost gap)\n- **Reasoning**: Premium solutions prove demand exists; aiDAPTIV+ democratizes access\n- **Action**: Position as \"affordable alternative to $7000 Mac Studio\"\n\n**Example Signals**:\n- \"Mac Studio M5 Ultra with 512GB unified memory\"\n- \"Apple charging $2400 for 512GB upgrade\"\n- \"Workstation market growing despite high prices\"\n\n---\n\n## ENTITY PRIORITY MATRIX\n\n### Tier 1 - Critical Monitoring (Daily)\n- **Partners**: Kioxia, Dell, HP\n- **Competitors**: Samsung, Silicon Motion, Marvell\n- **Influencers**: Jensen Huang (NVIDIA), Andrej Karpathy, major AI researchers\n\n### Tier 2 - Important Monitoring (Weekly)\n- **OEMs**: ASUS, Acer, Lenovo, Supermicro\n- **Memory Makers**: SK Hynix, Micron\n- **Analysts**: TrendForce, Gartner, IDC\n\n### Tier 3 - Background Monitoring (Monthly)\n- **Retailers**: Newegg, Micro Center\n- **Communities**: r/LocalLLaMA, HackerNews, GitHub\n- **Media**: AnandTech, Tom's Hardware, The Verge\n\n---\n\n## TOPIC KEYWORDS (Auto-Flag for Analysis)\n\n### High Priority\n- Memory shortage, DRAM shortage, HBM shortage\n- AI PC, edge AI, local AI, on-premises AI\n- Memory offload, SSD offload, tiered memory\n- Context window, KV cache, model size\n- Supply chain, component shortage, allocation\n\n### Medium Priority\n- PCIe Gen5, NVMe, SSD controller\n- LLM inference, model training, fine-tuning\n- Privacy, data sovereignty, cloud costs\n- NAND flash, 3D NAND, QLC, TLC\n\n### Low Priority (Context Only)\n- Datacenter AI, cloud AI (unless related to edge comparison)\n- Gaming GPUs (unless memory-related)\n- Mobile AI (different use case)\n\n---\n\n## OUTPUT EXPECTATIONS\n\nWhen analyzing intelligence, the system should:\n\n1. **Identify Relevance**: Does this news relate to Phison's strategic priorities?\n2. **Apply Context**: Which strategic rule(s) apply?\n3. **Determine Sentiment**: POSITIVE (Opportunity), NEGATIVE (Threat), NEUTRAL (Context), MIXED\n4. **Explain Reasoning**: Why does this matter to Phison specifically?\n5. **Suggest Action**: What should PMM team do with this insight?\n\n**Example Output**:\n```\nSource: TrendForce Report - Q1 2026 DRAM Forecast\nRelevance: HIGH\nStrategic Rule: #1 (Memory Constraints = Opportunity)\nSentiment: POSITIVE\nReasoning: 55-60% DRAM price increase will force OEMs to limit PC memory configurations, \ncreating demand for memory offload solutions like aiDAPTIV+.\nSuggested Action: Prepare messaging on \"aiDAPTIV+ as cost-effective alternative to expensive DRAM upgrades\"\n```\n",
      "added_at": 1768361322.1898181,
      "tokens": 1768
    },
    {
      "name": "intel_panther_lake_1767950563347.png",
      "category": "image",
      "size_kb": 568.7,
      "path": "/Users/rickallen/code/aidaptiv-cursor/data/realstatic/ces2026/images/infographics/intel_panther_lake_1767950563347.png",
      "content": "[Image: intel_panther_lake_1767950563347.png]",
      "added_at": 1768361406.17989,
      "tokens": 11
    },
    {
      "name": "samsung_ssd_roadmap_1767950527033.png",
      "category": "image",
      "size_kb": 721.5,
      "path": "/Users/rickallen/code/aidaptiv-cursor/data/realstatic/ces2026/images/infographics/samsung_ssd_roadmap_1767950527033.png",
      "content": "[Image: samsung_ssd_roadmap_1767950527033.png]",
      "added_at": 1768361629.5242639,
      "tokens": 11
    }
  ]
}